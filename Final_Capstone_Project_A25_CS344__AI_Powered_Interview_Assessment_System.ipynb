{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Install dan import Library**"
      ],
      "metadata": {
        "id": "-wLTL-0SShkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzS6aeT2hdVs",
        "outputId": "31cf7986-5135-432f-f285-843f297715d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-klg84cli\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-klg84cli\n",
            "  Resolved https://github.com/openai/whisper.git to commit c0d2f624c09dc18e709e37c2ad90c039a4eb72a2\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper==20250625) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper==20250625) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper==20250625) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper==20250625) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20250625) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper==20250625) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper==20250625) (3.0.3)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=90e30799127cbadecea6fc7b8f6f3b352d8a1b5e6fc4cd37e06452e91941b19c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gflx9ra5/wheels/c3/03/25/5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-sfRF67hhzf",
        "outputId": "5533371c-1c50-482d-eaba-e1a1a93cb2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.11.12)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ztg6pUJWopbchKWC4m2lqLH-0utWnJwI\n",
            "To: /content/payload.json\n",
            "100% 3.52k/3.52k [00:00<00:00, 12.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "file_id = \"1ztg6pUJWopbchKWC4m2lqLH-0utWnJwI\" #drive ke payload.json\n",
        "!gdown --id {file_id} -O payload.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzJSS5tCTIvV",
        "outputId": "76c14c41-d6c3-4515-b454-1afe43e0390e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer, evaluate\n",
            "Successfully installed evaluate-0.4.6 jiwer-4.0.0 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import gdown\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import pandas as pd\n",
        "import csv\n",
        "from jiwer import wer, cer, Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces, RemoveWhiteSpace"
      ],
      "metadata": {
        "id": "_ykVdY-RS0Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1032809e-481f-450d-d235-8419e4b42f54"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
            "  m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(flt)p?( \\(default\\))?$', token):\n",
            "/usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
            "  elif re.match('(dbl)p?( \\(default\\))?$', token):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJoazxmfQX5G",
        "outputId": "d8dd72bd-7897-4e9e-97c5-fc8aaf52059d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor"
      ],
      "metadata": {
        "id": "RxeQlB9PQdR2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oPlMWhpB_5z",
        "outputId": "b25c2c36-e67e-497f-cc2a-043679c0c699"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.37.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
            "Downloading groq-0.37.1-py3-none-any.whl (137 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from groq import Groq\n",
        "import time\n",
        "import pprint"
      ],
      "metadata": {
        "id": "javDFwAFaOYn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load dan Parsing Data**"
      ],
      "metadata": {
        "id": "pl09yJz5UIrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load dan Ekstrak Payload (.json)"
      ],
      "metadata": {
        "id": "EppNCRrsTbaF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BS22grgkQ3G",
        "outputId": "3534515c-50f1-44be-89f1-50f16a3bf12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video 1\n",
            "Q: Can you share any specific challenges you faced while working on certification and how you overcame them?\n",
            "Link: https://drive.google.com/file/d/1EY_uUFWdmjrWYP6H9NEnQa0hCrL_PQN5/view?usp=drive_link\n",
            "Video 2\n",
            "Q: Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\n",
            "Link: https://drive.google.com/file/d/12ZPALf3RXb-OwQ1FT78t6NnFDwfJXyqg/view?usp=drive_link\n",
            "Video 3\n",
            "Q: Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\n",
            "Link: https://drive.google.com/file/d/10ysNB7AvBd4986wlNDd0mHf86Hx0Rwy8/view?usp=drive_link\n",
            "Video 4\n",
            "Q: Explain how to implement dropout in a TensorFlow model and the effect it has on training.\n",
            "Link: https://drive.google.com/file/d/1dIU0q0oE_XHoeRBt9sQIpkxmtaioMhwx/view?usp=drive_link\n",
            "Video 5\n",
            "Q: Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\n",
            "Link: https://drive.google.com/file/d/1XCRRbgdVhART7s1dqBhLaF79sjzMwZNr/view?usp=drive_link\n"
          ]
        }
      ],
      "source": [
        "with open(\"payload.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mengambil list interview dari payload.json\n",
        "interviews = data[\"data\"][\"reviewChecklists\"][\"interviews\"]\n",
        "\n",
        "# Loop dan tampilkan detail tiap video yang berhasil diambil\n",
        "for i, item in enumerate(interviews, start=1):\n",
        "    print(f\"Video {i}\")\n",
        "    print(\"Q:\", item[\"question\"])\n",
        "    print(\"Link:\", item[\"recordedVideoUrl\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Download video"
      ],
      "metadata": {
        "id": "nAdeR4o4UMrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4A44Rcartiiy"
      },
      "outputs": [],
      "source": [
        "# Membuat folder videos\n",
        "os.makedirs(\"videos\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9ZLmm00nYbU",
        "outputId": "ed3cc227-3a43-40f2-90bd-715400456bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EY_uUFWdmjrWYP6H9NEnQa0hCrL_PQN5\n",
            "To: /content/videos/video_1.mp4\n",
            "100%|██████████| 18.0M/18.0M [00:00<00:00, 57.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12ZPALf3RXb-OwQ1FT78t6NnFDwfJXyqg\n",
            "To: /content/videos/video_2.mp4\n",
            "100%|██████████| 22.0M/22.0M [00:00<00:00, 28.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10ysNB7AvBd4986wlNDd0mHf86Hx0Rwy8\n",
            "To: /content/videos/video_3.mp4\n",
            "100%|██████████| 22.9M/22.9M [00:00<00:00, 48.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dIU0q0oE_XHoeRBt9sQIpkxmtaioMhwx\n",
            "To: /content/videos/video_4.mp4\n",
            "100%|██████████| 23.1M/23.1M [00:00<00:00, 60.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading video 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XCRRbgdVhART7s1dqBhLaF79sjzMwZNr\n",
            "To: /content/videos/video_5.mp4\n",
            "100%|██████████| 23.0M/23.0M [00:00<00:00, 34.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Semua video selesai di-download!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "video_infos = []\n",
        "\n",
        "for i, item in enumerate(interviews, 1):\n",
        "    question = item[\"question\"]\n",
        "    video_url = item[\"recordedVideoUrl\"]\n",
        "\n",
        "    # Ekstrak file dari google drive link\n",
        "    match = re.search(r\"/d/([a-zA-Z0-9_-]+)\", video_url)\n",
        "    if not match:\n",
        "        print(\"Tidak bisa menemukan file id untuk video:\", video_url)\n",
        "        continue\n",
        "\n",
        "    file_id = match.group(1)\n",
        "    filename = f\"videos/video_{i}.mp4\"\n",
        "\n",
        "    print(f\"Downloading video {i}\")\n",
        "    gdown.download(id=file_id, output=filename, quiet=False)\n",
        "\n",
        "    video_infos.append({\n",
        "        \"index\": i,\n",
        "        \"filename\": filename,\n",
        "        \"question\": question\n",
        "    })\n",
        "print(\"\\nSemua video selesai di-download!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ekstrak audio"
      ],
      "metadata": {
        "id": "oCsnaxLvUYW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install noisereduce soundfile"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8Uy3TdhxCvB",
        "outputId": "06898ad6-8b4c-476f-c9ea-7d0d490b7e4f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from noisereduce) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from noisereduce) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from noisereduce) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from noisereduce) (1.5.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Membuat folder untuk audios\n",
        "os.makedirs(\"audios\", exist_ok=True)"
      ],
      "metadata": {
        "id": "TSM4sy0GVyKz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub.effects import normalize\n",
        "from pydub import AudioSegment, silence\n",
        "\n",
        "for info in video_infos:\n",
        "    video_path = info[\"filename\"]\n",
        "    audio_path = f\"audios/audio_{info['index']}.wav\"\n",
        "\n",
        "    print(f\"Extracting audio for {video_path}...\")\n",
        "\n",
        "    # 1. Extract audio dari video\n",
        "    audio = AudioSegment.from_file(video_path)\n",
        "    audio = normalize(audio)\n",
        "    audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "    audio.export(audio_path, format=\"wav\")\n",
        "\n",
        "    # 2. Load kembali dan bersihkan gumaman / segments pelan\n",
        "    clean_audio = AudioSegment.from_wav(audio_path)\n",
        "    chunks = silence.split_on_silence(\n",
        "        clean_audio,\n",
        "        min_silence_len=300,\n",
        "        silence_thresh=-35  # semakin besar, semakin tidak mentolerir suara pelan\n",
        "    )\n",
        "\n",
        "    # 3. Jika ada chunk keras → gabungkan\n",
        "    if len(chunks) > 0:\n",
        "        cleaned = sum(chunks)\n",
        "        cleaned.export(audio_path, format=\"wav\")\n",
        "\n",
        "    # Simpan path-nya\n",
        "    info[\"audio\"] = audio_path\n",
        "\n",
        "print(\"\\nSemua audio .wav selesai dibuat!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5J7ZSOsB8sp",
        "outputId": "3f3c180a-0011-4ece-a353-a27854db112c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting audio for videos/video_1.mp4...\n",
            "Extracting audio for videos/video_2.mp4...\n",
            "Extracting audio for videos/video_3.mp4...\n",
            "Extracting audio for videos/video_4.mp4...\n",
            "Extracting audio for videos/video_5.mp4...\n",
            "\n",
            "Semua audio .wav selesai dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PERBANDINGAN MODEL**"
      ],
      "metadata": {
        "id": "u9r41tfTGWU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Whisper**"
      ],
      "metadata": {
        "id": "grG2K0szZJ9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung confidence score STT\n",
        "def whisper_confidence(result):\n",
        "    segments = result.get(\"segments\", [])\n",
        "    if not segments:\n",
        "        return 0.0\n",
        "\n",
        "    logprobs = []\n",
        "    for seg in segments:\n",
        "        lp = seg.get(\"avg_logprob\", None)\n",
        "        if lp is not None:\n",
        "            logprobs.append(lp)\n",
        "\n",
        "    if not logprobs:\n",
        "        return 0.0\n",
        "\n",
        "    mean_lp = sum(logprobs) / len(logprobs)\n",
        "\n",
        "    # Asumsi range logprob -5 (jelek) dan 0 (bagus)\n",
        "    min_lp, max_lp = -5.0, 0.0\n",
        "    mean_lp = max(min_lp, min(max_lp, mean_lp))\n",
        "\n",
        "    # Normalisasi ke 0–1\n",
        "    conf_0_1 = (mean_lp - min_lp) / (max_lp - min_lp)\n",
        "    return round(conf_0_1 * 100.0, 2)"
      ],
      "metadata": {
        "id": "K30qq-Mwdwbs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "N3Kz50fyklV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1caf65d-56ef-4e9d-feb9-bb89d243aedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.87G/2.87G [00:22<00:00, 140MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Can you share any specific challenges you faced while working on certification and how you overcame them?\n",
            "Transcript           :  Can you share any specific challenges faced while working on Skyshine and how you overcome them? Ah, okay. Actually, for the challenges, there are some challenges when I took the certifications, especially for the projects I mentioned that I already working with it. The first one is actually to meet the specific accuracy or validation loss for the evaluation matrix. And yeah, actually, that's just need to take some trial and error with different architecture. For example, like we can try to add more layer, more neurons, changes the neurons. Or even I also apply the dropout layer. So yeah, it really helps with the validation loss to become more lower, right? And yeah, I think that's one of the biggest challenges that I have while working on this certification.\n",
            "Confidence (0–100)   : 91.19%\n",
            "--------------------------------------------------\n",
            "Question 2: Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\n",
            "Transcript           :  Can you describe your experience with transfer learning and TensorFlow, how do you benefit from it? Ah, okay. About transfer learning is actually, we use existing trained model from TensorFlow, for example, like VGC16, VGC19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is, that's already a trained model with exceptionally high accuracy, high performance. Yeah, even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGC19, VGC16, EfficientNet, it will help to improve our models, comparing to the one if you use a traditional CNN model. So, yeah, CNN model with the convolutional 2D, yeah, max pooling, and yeah, it's quite good, actually, to use transfer learning. It really helps with our model performance, to improve our model performance.\n",
            "Confidence (0–100)   : 92.71%\n",
            "--------------------------------------------------\n",
            "Question 3: Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\n",
            "Transcript           :  Describe a core model you've built and the steps you took to ensure accuracy and efficiency. Complex transfer model you've built to ensure accuracy. Okay, I will take one of my previous projects that I used. I also used Keras transfer model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my scripts. I used this model. It's quite challenging. Even though it's achieved high accuracy with some dense layer, with some drawout layer. And trial and error also with the callback function with the neurons. But the problem is the dataset is not balanced. So it has the imbalanced class dataset. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Which is basically just oversampling and undersampling the dataset. It helps with the accuracy.\n",
            "Confidence (0–100)   : 91.45%\n",
            "--------------------------------------------------\n",
            "Question 4: Explain how to implement dropout in a TensorFlow model and the effect it has on training.\n",
            "Transcript           :  Explain how to implement dropout in test server model and test on training. Previously, I also have implemented the dropout layer also in the project function within this certifications and we can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the one that the case is image classifications, yeah, a German traffic something, if I'm not wrong, I have used this dropout layer in the, not in the last, in the middle of the layer, so there's a flattened layer, right, not flattened, the convolutional layer and the flattened layer and I use that dropout layer which is I use with the rate of 0.2 or 0.5, if I'm not wrong, and then the dense layer and the last, the output layer, right, the effect is it will really help to improve our accuracy and lower our validation loss by turning off some of the previous layer, yeah, for example, like we have a dense layer before and the next layer, we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch for the previous dense layer. Okay.\n",
            "Confidence (0–100)   : 90.15%\n",
            "--------------------------------------------------\n",
            "Question 5: Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\n",
            "Transcript           :  Describe the process of building a new section, okay. The CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class, okay? And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset, right? Yeah, of course, we can use another set, which is the test dataset, yeah. But, yeah, okay, the next one, we can just, maybe we need to implement also the image augmentation, yeah, data image augmentation to make our dataset more veritative, right? For example, like, we can rotate, we can zoom it, we can crop it, yeah. And, yeah, the last thing, yeah, of course, we can build our chain model with the Convolutional 2D, specify the filters, the kernel size, the LRM definition, of course, the input shape for the first layer, and then we can apply the Max Pooling 2D, yeah, and the next layer, we can just use Convolutional 2D, Max Pooling, and whatever it is. And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dash layer, right, for the output, the last output layer.\n",
            "Confidence (0–100)   : 91.75%\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = whisper.load_model(\"large-v2\")\n",
        "rows = []\n",
        "transcripts = {}\n",
        "\n",
        "for info in video_infos:\n",
        "    result = model.transcribe(\n",
        "    info[\"audio\"],\n",
        "    language=\"en\",\n",
        "    task=\"transcribe\",\n",
        "    fp16=False,\n",
        "    temperature=0.0,\n",
        "    best_of=7,  # Ambil hasil terbaik dari 7 percobaan\n",
        "    beam_size=5,\n",
        "    word_timestamps=True)\n",
        "\n",
        "    text = result[\"text\"]\n",
        "    stt_conf = whisper_confidence(result)\n",
        "    print(f\"Question {info['index']}: {info['question']}\")\n",
        "    print(f\"Transcript           : {text}\")\n",
        "    print(f\"Confidence (0–100)   : {stt_conf}%\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    transcripts[f\"question_{info['index']}\"] = {\n",
        "        \"question\": info[\"question\"],\n",
        "        \"transcript\": text,\n",
        "        \"stt_confidence\": round(stt_conf, 2)}\n",
        "\n",
        "    rows.append({\n",
        "        \"index\": info[\"index\"],\n",
        "        \"question\": info[\"question\"],\n",
        "        \"audio_file\": info[\"audio\"],\n",
        "        \"transcript_auto\": text,\n",
        "         \"stt_confidence\": stt_conf\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untuk Import ke transripts.json\n",
        "\n",
        "with open(\"transcripts.json\", \"w\") as f:\n",
        "    json.dump(transcripts, f, indent=2)\n",
        "\n",
        "print(\"Semua transkrip berhasil disimpan ke transcripts.json\")"
      ],
      "metadata": {
        "id": "ryhg0fO4LWcA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861e71bf-c4a1-4dfa-f224-9de47358d16e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semua transkrip berhasil disimpan ke transcripts.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(rows)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hz92EZcXZzJl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ff4c13c5-0ab9-4c6d-9e00-3f70463a32d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                           question  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "           audio_file                                    transcript_auto  \\\n",
              "0  audios/audio_1.wav   Can you share any specific challenges faced w...   \n",
              "1  audios/audio_2.wav   Can you describe your experience with transfe...   \n",
              "2  audios/audio_3.wav   Describe a core model you've built and the st...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in test serv...   \n",
              "4  audios/audio_5.wav   Describe the process of building a new sectio...   \n",
              "\n",
              "   stt_confidence  \n",
              "0           91.19  \n",
              "1           92.71  \n",
              "2           91.45  \n",
              "3           90.15  \n",
              "4           91.75  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebdf4648-d4be-4dc2-971e-adf14a64c18d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>audio_file</th>\n",
              "      <th>transcript_auto</th>\n",
              "      <th>stt_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can you share any specific challenges faced w...</td>\n",
              "      <td>91.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can you describe your experience with transfe...</td>\n",
              "      <td>92.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a core model you've built and the st...</td>\n",
              "      <td>91.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in test serv...</td>\n",
              "      <td>90.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>Describe the process of building a new sectio...</td>\n",
              "      <td>91.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebdf4648-d4be-4dc2-971e-adf14a64c18d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ebdf4648-d4be-4dc2-971e-adf14a64c18d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ebdf4648-d4be-4dc2-971e-adf14a64c18d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2b5dc014-43eb-4b0c-880d-31f6d70968bc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2b5dc014-43eb-4b0c-880d-31f6d70968bc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2b5dc014-43eb-4b0c-880d-31f6d70968bc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_auto\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can you describe your experience with transfer learning and TensorFlow, how do you benefit from it? Ah, okay. About transfer learning is actually, we use existing trained model from TensorFlow, for example, like VGC16, VGC19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is, that's already a trained model with exceptionally high accuracy, high performance. Yeah, even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGC19, VGC16, EfficientNet, it will help to improve our models, comparing to the one if you use a traditional CNN model. So, yeah, CNN model with the convolutional 2D, yeah, max pooling, and yeah, it's quite good, actually, to use transfer learning. It really helps with our model performance, to improve our model performance.\",\n          \" Describe the process of building a new section, okay. The CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class, okay? And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset, right? Yeah, of course, we can use another set, which is the test dataset, yeah. But, yeah, okay, the next one, we can just, maybe we need to implement also the image augmentation, yeah, data image augmentation to make our dataset more veritative, right? For example, like, we can rotate, we can zoom it, we can crop it, yeah. And, yeah, the last thing, yeah, of course, we can build our chain model with the Convolutional 2D, specify the filters, the kernel size, the LRM definition, of course, the input shape for the first layer, and then we can apply the Max Pooling 2D, yeah, and the next layer, we can just use Convolutional 2D, Max Pooling, and whatever it is. And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dash layer, right, for the output, the last output layer.\",\n          \" Describe a core model you've built and the steps you took to ensure accuracy and efficiency. Complex transfer model you've built to ensure accuracy. Okay, I will take one of my previous projects that I used. I also used Keras transfer model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my scripts. I used this model. It's quite challenging. Even though it's achieved high accuracy with some dense layer, with some drawout layer. And trial and error also with the callback function with the neurons. But the problem is the dataset is not balanced. So it has the imbalanced class dataset. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Which is basically just oversampling and undersampling the dataset. It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stt_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9267146270562436,\n        \"min\": 90.15,\n        \"max\": 92.71,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          92.71,\n          91.75,\n          91.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "307EIQUqk4FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d814ff95-d3cb-4785-ecc3-e52817883318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_transcript_whisper.csv berhasil dibuat!\n"
          ]
        }
      ],
      "source": [
        "df.to_csv(\"dataset_transcript_whisper.csv\", index=False)\n",
        "print(\"dataset_transcript_whisper.csv berhasil dibuat!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merging CSV"
      ],
      "metadata": {
        "id": "9T-Hzs1TKNAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    [\"index\",\"question\",\"audio_file\",\"transcript_manual\"],\n",
        "    [1,\"Can you share any specific challenges you faced while working on certification and how you overcame them?\",\"audios/audio_1.wav\",\" Can share any specific challenges you faced while working on certification and how you overcome them ah, ok, actually, uh for the challenges, uh there are some challenges uh when uh i took the certification especially for the the projects i mentioned that uh that i uh i already working uh with it uh the first one is actually uh to to meet to meet yeah to meet the specific accuracy or uh validation loss right for the evaluation matrix and yeah actually that's just need to to take some trial and error uh with with different architecture for example like uh we can uh try to add more uh layer, more neurons, changes the neurons or even uh i also apply the dropout layer so yeah it really helps with the the validation loss to become more lower right and yeah i think that's one of the biggest eh challenges that that i have uh while uh working on this certifications.\"],\n",
        "    [2,\"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\"audios/audio_2.wav\",\" Can describe your experience with transfer learning in tensorflow? How do you benefit from the projects? ah, ok, uh About transfer learning is actually uh we use existing train model from tensorflow for example uh like VGG16, VGG19 right uh especially uh for for some cases uh that we need to use deep learning using Keras applications uh for example like image classification, we can use transfer learning uh models which is that's already uh trained model with uh exceptionally uh high accuracy high performance uh yeah even though it's trained with different uh datasets but it it really helps uh to improve uh our uh model performance uh, model accuracy, model loss uh and yeah uh for example like mobile net, VGG19, VGG16 ya, efficient net uh. It will help to improve our our models uh comparing to the one if you use a traditional uh CNN model yeah CNN model uh with the convolutional uh 2d yeah max pooling and yeah it's it's quite good actually to use uh transfer learning it really helps uh with our model performance to improve our model performance.\"],\n",
        "    [3,\"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\",\"audios/audio_3.wav\",\" Describe a complex tensorflow model you've built and the steps you took to ensure its accuracy and efficiency hm, uh complex tensorflow model you've built and steps you took to ensure its accuracy hm, ok, uh i will take one of my uh previous project that uh i use, yeah i also use keras tensorflow model uh its it is uh about uh celiac disease prediction ya uh yeah, this is also i use uh the research project for my uh undergraduate thesis yeah for my skripsi and uh i use this model, it's quite challenging uh even though it's uh achieved high accuracy yeah with with some dense layer yeah, with some uh dropout layer and trial and error with the uh also with the uh callback function yeah uh with the neurons but the problem is the dataset is not uh balance yeah so it has the imbalanced uh class datasets yeah and uh the the approach that i use is to yeah just to use uh the technique called smote and yeah synthetic oversampling technique with edited nearest neighbor. yeah which is, uh Basically it's just oversampling and undersampling uh the datasets. uh It helps with the accuracy.\"],\n",
        "    [4,\"Explain how to implement dropout in a TensorFlow model and the effect it has on training.\",\"audios/audio_4.wav\",\" Explain how to implement dropout in ah ok dropout in tensorflow model and it test on training. hm Previously, uh I also have uh implement the dropout layer yeah also in the project submission uh within this certifications. and We can just add the dropout layer. uh For example, uh if I'm not mistaken, I have used this dropout layer in the the one that the case is hm yeah image classifications yeah. German traffic, something if I'm not wrong. I have used this dropout layer uh in in in the not in the last uh in the middle in the middle of the layer. uh So there there's a flattened layer right not flattened, the convolutional layer and the flattened layer. uh and I use that dropout layer, uh uh which is uh I used with the rate of 0.2 or 0.5 if if I'm not wrong. uh And then the dense layer and the the last, the output layer right. uh The effect is it will really helps to hm improve our accuracy and lower our validation loss uh by by turning off some of the previous layer. yeah For example like, we have dense layer 64 and the next layer, uh we implement the dropout uh layer with the rate of uh o point five and it will uh turn off randomly each epoch uh for of the previous dense layer uh\"],\n",
        "    [5,\"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\"audios/audio_5.wav\",\" describe the process of building image, okay uh the cnn one, right so, uh at the first time uh of course uh, we need to make sure uh there are split uh the the image folder split for for uh each uh class okay, uh and then we can use uh Keras preprocessing if I'm not mistaken uh image dataset from directory to split the training and the validation uh dataset right, uh yeah of course we can use another another set which is the test dataset, yeah uh but yeah, okay the next one uh we can, uh yeah we can just uh maybe we we need to implement also the image augmentation yeah, data uh data image data augmentation to uh to make our dataset more uh variative, right for example like we can rotate we can zoom it, we can uh crop it yeah, uh and yeah the last thing, yeah of course we we can build our gen model with the convolutional 2d uh specify the filters, the kernel size, the LRF division of course the input the input shape for the first layer and then we can apply the max pooling 2d uh yeah, and the next layer we can just use convolutional 2D, max pooling and uh whatever it is uh and after that uh we apply the flatten layer uh and dropout layer if you want and the last thing uh don't forget to use the dense layer, right for the output, like uh the last.\"]\n",
        "    ]\n",
        "\n",
        "with open(\"ground_truth.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(data)\n",
        "\n",
        "print(\"CSV berhasil dibuat!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQg2J3tJ0Uu1",
        "outputId": "568e8793-6d8a-456f-cd65-7654ffcde0e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV berhasil dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_manual = pd.read_csv(\"/content/ground_truth.csv\")\n",
        "df_manual.head()"
      ],
      "metadata": {
        "id": "1LfCEcuvMGRE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a2e126da-6840-4e91-e182-0279e8fd7fbc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                           question  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "           audio_file                                  transcript_manual  \n",
              "0  audios/audio_1.wav   Can share any specific challenges you faced w...  \n",
              "1  audios/audio_2.wav   Can describe your experience with transfer le...  \n",
              "2  audios/audio_3.wav   Describe a complex tensorflow model you've bu...  \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in ah ok dro...  \n",
              "4  audios/audio_5.wav   describe the process of building image, okay ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4f3568b-f7ff-4a84-9500-32a22625eb9c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>audio_file</th>\n",
              "      <th>transcript_manual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can share any specific challenges you faced w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can describe your experience with transfer le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a complex tensorflow model you've bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in ah ok dro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>describe the process of building image, okay ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4f3568b-f7ff-4a84-9500-32a22625eb9c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4f3568b-f7ff-4a84-9500-32a22625eb9c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4f3568b-f7ff-4a84-9500-32a22625eb9c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c44f816a-e412-4f8f-9574-a273a3f1969f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c44f816a-e412-4f8f-9574-a273a3f1969f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c44f816a-e412-4f8f-9574-a273a3f1969f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_manual",
              "summary": "{\n  \"name\": \"df_manual\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can describe your experience with transfer learning in tensorflow? How do you benefit from the projects? ah, ok, uh About transfer learning is actually uh we use existing train model from tensorflow for example uh like VGG16, VGG19 right uh especially uh for for some cases uh that we need to use deep learning using Keras applications uh for example like image classification, we can use transfer learning uh models which is that's already uh trained model with uh exceptionally uh high accuracy high performance uh yeah even though it's trained with different uh datasets but it it really helps uh to improve uh our uh model performance uh, model accuracy, model loss uh and yeah uh for example like mobile net, VGG19, VGG16 ya, efficient net uh. It will help to improve our our models uh comparing to the one if you use a traditional uh CNN model yeah CNN model uh with the convolutional uh 2d yeah max pooling and yeah it's it's quite good actually to use uh transfer learning it really helps uh with our model performance to improve our model performance.\",\n          \" describe the process of building image, okay uh the cnn one, right so, uh at the first time uh of course uh, we need to make sure uh there are split uh the the image folder split for for uh each uh class okay, uh and then we can use uh Keras preprocessing if I'm not mistaken uh image dataset from directory to split the training and the validation uh dataset right, uh yeah of course we can use another another set which is the test dataset, yeah uh but yeah, okay the next one uh we can, uh yeah we can just uh maybe we we need to implement also the image augmentation yeah, data uh data image data augmentation to uh to make our dataset more uh variative, right for example like we can rotate we can zoom it, we can uh crop it yeah, uh and yeah the last thing, yeah of course we we can build our gen model with the convolutional 2d uh specify the filters, the kernel size, the LRF division of course the input the input shape for the first layer and then we can apply the max pooling 2d uh yeah, and the next layer we can just use convolutional 2D, max pooling and uh whatever it is uh and after that uh we apply the flatten layer uh and dropout layer if you want and the last thing uh don't forget to use the dense layer, right for the output, like uh the last.\",\n          \" Describe a complex tensorflow model you've built and the steps you took to ensure its accuracy and efficiency hm, uh complex tensorflow model you've built and steps you took to ensure its accuracy hm, ok, uh i will take one of my uh previous project that uh i use, yeah i also use keras tensorflow model uh its it is uh about uh celiac disease prediction ya uh yeah, this is also i use uh the research project for my uh undergraduate thesis yeah for my skripsi and uh i use this model, it's quite challenging uh even though it's uh achieved high accuracy yeah with with some dense layer yeah, with some uh dropout layer and trial and error with the uh also with the uh callback function yeah uh with the neurons but the problem is the dataset is not uh balance yeah so it has the imbalanced uh class datasets yeah and uh the the approach that i use is to yeah just to use uh the technique called smote and yeah synthetic oversampling technique with edited nearest neighbor. yeah which is, uh Basically it's just oversampling and undersampling uh the datasets. uh It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi + hapus filler words\n",
        "transform = Compose([\n",
        "    ToLowerCase(),\n",
        "    RemovePunctuation(),\n",
        "    RemoveMultipleSpaces(),\n",
        "    RemoveWhiteSpace(replace_by_space=True),\n",
        "])\n",
        "\n",
        "df_manual[\"transcript_manual_cleaned\"] = df_manual[\"transcript_manual\"].apply(transform)\n",
        "\n",
        "filler_words = r\"\\b(uh|hmm|oke|ya|yeah|okay|huh|emm|hm|ah|ok)\\b\"\n",
        "df_manual[\"transcript_manual_cleaned\"] = df_manual[\"transcript_manual_cleaned\"].apply(\n",
        "    lambda text: re.sub(filler_words, '', text, flags=re.IGNORECASE))\n",
        "\n",
        "df_manual.to_csv(\"ground_truth_cleaned.csv\", index=False)\n",
        "print(\"ground_truth_cleaned.csv berhasil dibuat\")"
      ],
      "metadata": {
        "id": "xz8KP98Wzewo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a33fbe-26a8-49db-f001-6098e24bbf35"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ground_truth_cleaned.csv berhasil dibuat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_manual = pd.read_csv(\"/content/ground_truth_cleaned.csv\")\n",
        "df_manual.head()"
      ],
      "metadata": {
        "id": "f4cHm2jN-2eB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ee512440-ffce-410b-ff9f-6a60d5d7cf79"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                           question  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "           audio_file                                  transcript_manual  \\\n",
              "0  audios/audio_1.wav   Can share any specific challenges you faced w...   \n",
              "1  audios/audio_2.wav   Can describe your experience with transfer le...   \n",
              "2  audios/audio_3.wav   Describe a complex tensorflow model you've bu...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in ah ok dro...   \n",
              "4  audios/audio_5.wav   describe the process of building image, okay ...   \n",
              "\n",
              "                           transcript_manual_cleaned  \n",
              "0   can share any specific challenges you faced w...  \n",
              "1   can describe your experience with transfer le...  \n",
              "2   describe a complex tensorflow model youve bui...  \n",
              "3   explain how to implement dropout in   dropout...  \n",
              "4   describe the process of building image   the ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7ac2bd57-b0c1-4d57-860b-431ec0d85b5a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>audio_file</th>\n",
              "      <th>transcript_manual</th>\n",
              "      <th>transcript_manual_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can share any specific challenges you faced w...</td>\n",
              "      <td>can share any specific challenges you faced w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can describe your experience with transfer le...</td>\n",
              "      <td>can describe your experience with transfer le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a complex tensorflow model you've bu...</td>\n",
              "      <td>describe a complex tensorflow model youve bui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in ah ok dro...</td>\n",
              "      <td>explain how to implement dropout in   dropout...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>describe the process of building image, okay ...</td>\n",
              "      <td>describe the process of building image   the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ac2bd57-b0c1-4d57-860b-431ec0d85b5a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7ac2bd57-b0c1-4d57-860b-431ec0d85b5a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7ac2bd57-b0c1-4d57-860b-431ec0d85b5a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d8ea983b-f5dd-4f8b-b96d-ba035505b001\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8ea983b-f5dd-4f8b-b96d-ba035505b001')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d8ea983b-f5dd-4f8b-b96d-ba035505b001 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_manual",
              "summary": "{\n  \"name\": \"df_manual\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can describe your experience with transfer learning in tensorflow? How do you benefit from the projects? ah, ok, uh About transfer learning is actually uh we use existing train model from tensorflow for example uh like VGG16, VGG19 right uh especially uh for for some cases uh that we need to use deep learning using Keras applications uh for example like image classification, we can use transfer learning uh models which is that's already uh trained model with uh exceptionally uh high accuracy high performance uh yeah even though it's trained with different uh datasets but it it really helps uh to improve uh our uh model performance uh, model accuracy, model loss uh and yeah uh for example like mobile net, VGG19, VGG16 ya, efficient net uh. It will help to improve our our models uh comparing to the one if you use a traditional uh CNN model yeah CNN model uh with the convolutional uh 2d yeah max pooling and yeah it's it's quite good actually to use uh transfer learning it really helps uh with our model performance to improve our model performance.\",\n          \" describe the process of building image, okay uh the cnn one, right so, uh at the first time uh of course uh, we need to make sure uh there are split uh the the image folder split for for uh each uh class okay, uh and then we can use uh Keras preprocessing if I'm not mistaken uh image dataset from directory to split the training and the validation uh dataset right, uh yeah of course we can use another another set which is the test dataset, yeah uh but yeah, okay the next one uh we can, uh yeah we can just uh maybe we we need to implement also the image augmentation yeah, data uh data image data augmentation to uh to make our dataset more uh variative, right for example like we can rotate we can zoom it, we can uh crop it yeah, uh and yeah the last thing, yeah of course we we can build our gen model with the convolutional 2d uh specify the filters, the kernel size, the LRF division of course the input the input shape for the first layer and then we can apply the max pooling 2d uh yeah, and the next layer we can just use convolutional 2D, max pooling and uh whatever it is uh and after that uh we apply the flatten layer uh and dropout layer if you want and the last thing uh don't forget to use the dense layer, right for the output, like uh the last.\",\n          \" Describe a complex tensorflow model you've built and the steps you took to ensure its accuracy and efficiency hm, uh complex tensorflow model you've built and steps you took to ensure its accuracy hm, ok, uh i will take one of my uh previous project that uh i use, yeah i also use keras tensorflow model uh its it is uh about uh celiac disease prediction ya uh yeah, this is also i use uh the research project for my uh undergraduate thesis yeah for my skripsi and uh i use this model, it's quite challenging uh even though it's uh achieved high accuracy yeah with with some dense layer yeah, with some uh dropout layer and trial and error with the uh also with the uh callback function yeah uh with the neurons but the problem is the dataset is not uh balance yeah so it has the imbalanced uh class datasets yeah and uh the the approach that i use is to yeah just to use uh the technique called smote and yeah synthetic oversampling technique with edited nearest neighbor. yeah which is, uh Basically it's just oversampling and undersampling uh the datasets. uh It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" can describe your experience with transfer learning in tensorflow how do you benefit from the projects    about transfer learning is actually  we use existing train model from tensorflow for example  like vgg16 vgg19 right  especially  for for some cases  that we need to use deep learning using keras applications  for example like image classification we can use transfer learning  models which is thats already  trained model with  exceptionally  high accuracy high performance   even though its trained with different  datasets but it it really helps  to improve  our  model performance  model accuracy model loss  and   for example like mobile net vgg19 vgg16  efficient net  it will help to improve our our models  comparing to the one if you use a traditional  cnn model  cnn model  with the convolutional  2d  max pooling and  its its quite good actually to use  transfer learning it really helps  with our model performance to improve our model performance\",\n          \" describe the process of building image   the cnn one right so  at the first time  of course  we need to make sure  there are split  the the image folder split for for  each  class   and then we can use  keras preprocessing if im not mistaken  image dataset from directory to split the training and the validation  dataset right   of course we can use another another set which is the test dataset   but   the next one  we can   we can just  maybe we we need to implement also the image augmentation  data  data image data augmentation to  to make our dataset more  variative right for example like we can rotate we can zoom it we can  crop it   and  the last thing  of course we we can build our gen model with the convolutional 2d  specify the filters the kernel size the lrf division of course the input the input shape for the first layer and then we can apply the max pooling 2d   and the next layer we can just use convolutional 2d max pooling and  whatever it is  and after that  we apply the flatten layer  and dropout layer if you want and the last thing  dont forget to use the dense layer right for the output like  the last\",\n          \" describe a complex tensorflow model youve built and the steps you took to ensure its accuracy and efficiency   complex tensorflow model youve built and steps you took to ensure its accuracy    i will take one of my  previous project that  i use  i also use keras tensorflow model  its it is  about  celiac disease prediction    this is also i use  the research project for my  undergraduate thesis  for my skripsi and  i use this model its quite challenging  even though its  achieved high accuracy  with with some dense layer  with some  dropout layer and trial and error with the  also with the  callback function   with the neurons but the problem is the dataset is not  balance  so it has the imbalanced  class datasets  and  the the approach that i use is to  just to use  the technique called smote and  synthetic oversampling technique with edited nearest neighbor  which is  basically its just oversampling and undersampling  the datasets  it helps with the accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_auto = pd.read_csv(\"/content/dataset_transcript_whisper.csv\")\n",
        "df_auto.head()"
      ],
      "metadata": {
        "id": "dBVawirdM8ZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "18278c1e-bbf4-4f08-afe1-9075227308e9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                           question  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "           audio_file                                    transcript_auto  \\\n",
              "0  audios/audio_1.wav   Can you share any specific challenges faced w...   \n",
              "1  audios/audio_2.wav   Can you describe your experience with transfe...   \n",
              "2  audios/audio_3.wav   Describe a core model you've built and the st...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in test serv...   \n",
              "4  audios/audio_5.wav   Describe the process of building a new sectio...   \n",
              "\n",
              "   stt_confidence  \n",
              "0           91.19  \n",
              "1           92.71  \n",
              "2           91.45  \n",
              "3           90.15  \n",
              "4           91.75  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b759b2f9-c6ba-4b88-ad5f-e01610f701f7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>audio_file</th>\n",
              "      <th>transcript_auto</th>\n",
              "      <th>stt_confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can you share any specific challenges faced w...</td>\n",
              "      <td>91.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can you describe your experience with transfe...</td>\n",
              "      <td>92.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a core model you've built and the st...</td>\n",
              "      <td>91.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in test serv...</td>\n",
              "      <td>90.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>Describe the process of building a new sectio...</td>\n",
              "      <td>91.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b759b2f9-c6ba-4b88-ad5f-e01610f701f7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b759b2f9-c6ba-4b88-ad5f-e01610f701f7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b759b2f9-c6ba-4b88-ad5f-e01610f701f7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-11c40c36-8b99-478a-92c2-4605eeba6a5b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11c40c36-8b99-478a-92c2-4605eeba6a5b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-11c40c36-8b99-478a-92c2-4605eeba6a5b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_auto",
              "summary": "{\n  \"name\": \"df_auto\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_auto\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can you describe your experience with transfer learning and TensorFlow, how do you benefit from it? Ah, okay. About transfer learning is actually, we use existing trained model from TensorFlow, for example, like VGC16, VGC19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is, that's already a trained model with exceptionally high accuracy, high performance. Yeah, even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGC19, VGC16, EfficientNet, it will help to improve our models, comparing to the one if you use a traditional CNN model. So, yeah, CNN model with the convolutional 2D, yeah, max pooling, and yeah, it's quite good, actually, to use transfer learning. It really helps with our model performance, to improve our model performance.\",\n          \" Describe the process of building a new section, okay. The CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class, okay? And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset, right? Yeah, of course, we can use another set, which is the test dataset, yeah. But, yeah, okay, the next one, we can just, maybe we need to implement also the image augmentation, yeah, data image augmentation to make our dataset more veritative, right? For example, like, we can rotate, we can zoom it, we can crop it, yeah. And, yeah, the last thing, yeah, of course, we can build our chain model with the Convolutional 2D, specify the filters, the kernel size, the LRM definition, of course, the input shape for the first layer, and then we can apply the Max Pooling 2D, yeah, and the next layer, we can just use Convolutional 2D, Max Pooling, and whatever it is. And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dash layer, right, for the output, the last output layer.\",\n          \" Describe a core model you've built and the steps you took to ensure accuracy and efficiency. Complex transfer model you've built to ensure accuracy. Okay, I will take one of my previous projects that I used. I also used Keras transfer model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my scripts. I used this model. It's quite challenging. Even though it's achieved high accuracy with some dense layer, with some drawout layer. And trial and error also with the callback function with the neurons. But the problem is the dataset is not balanced. So it has the imbalanced class dataset. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Which is basically just oversampling and undersampling the dataset. It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stt_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9267146270562436,\n        \"min\": 90.15,\n        \"max\": 92.71,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          92.71,\n          91.75,\n          91.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merge = df_auto.merge(df_manual, on=\"index\", how=\"inner\")\n",
        "\n",
        "print(\"Dataset gabungan:\")\n",
        "print(df_merge.columns)\n",
        "df_merge.head()"
      ],
      "metadata": {
        "id": "tQjPbO03NAP_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "2099e89e-7f74-4420-ce0b-18542bc85dac"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset gabungan:\n",
            "Index(['index', 'question_x', 'audio_file_x', 'transcript_auto',\n",
            "       'stt_confidence', 'question_y', 'audio_file_y', 'transcript_manual',\n",
            "       'transcript_manual_cleaned'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                         question_x  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "         audio_file_x                                    transcript_auto  \\\n",
              "0  audios/audio_1.wav   Can you share any specific challenges faced w...   \n",
              "1  audios/audio_2.wav   Can you describe your experience with transfe...   \n",
              "2  audios/audio_3.wav   Describe a core model you've built and the st...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in test serv...   \n",
              "4  audios/audio_5.wav   Describe the process of building a new sectio...   \n",
              "\n",
              "   stt_confidence                                         question_y  \\\n",
              "0           91.19  Can you share any specific challenges you face...   \n",
              "1           92.71  Can you describe your experience with transfer...   \n",
              "2           91.45  Describe a complex TensorFlow model you have b...   \n",
              "3           90.15  Explain how to implement dropout in a TensorFl...   \n",
              "4           91.75  Describe the process of building a convolution...   \n",
              "\n",
              "         audio_file_y                                  transcript_manual  \\\n",
              "0  audios/audio_1.wav   Can share any specific challenges you faced w...   \n",
              "1  audios/audio_2.wav   Can describe your experience with transfer le...   \n",
              "2  audios/audio_3.wav   Describe a complex tensorflow model you've bu...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in ah ok dro...   \n",
              "4  audios/audio_5.wav   describe the process of building image, okay ...   \n",
              "\n",
              "                           transcript_manual_cleaned  \n",
              "0   can share any specific challenges you faced w...  \n",
              "1   can describe your experience with transfer le...  \n",
              "2   describe a complex tensorflow model youve bui...  \n",
              "3   explain how to implement dropout in   dropout...  \n",
              "4   describe the process of building image   the ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32456cbf-ca09-4702-86f9-925bb345557e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question_x</th>\n",
              "      <th>audio_file_x</th>\n",
              "      <th>transcript_auto</th>\n",
              "      <th>stt_confidence</th>\n",
              "      <th>question_y</th>\n",
              "      <th>audio_file_y</th>\n",
              "      <th>transcript_manual</th>\n",
              "      <th>transcript_manual_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can you share any specific challenges faced w...</td>\n",
              "      <td>91.19</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can share any specific challenges you faced w...</td>\n",
              "      <td>can share any specific challenges you faced w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can you describe your experience with transfe...</td>\n",
              "      <td>92.71</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can describe your experience with transfer le...</td>\n",
              "      <td>can describe your experience with transfer le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a core model you've built and the st...</td>\n",
              "      <td>91.45</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a complex tensorflow model you've bu...</td>\n",
              "      <td>describe a complex tensorflow model youve bui...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in test serv...</td>\n",
              "      <td>90.15</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in ah ok dro...</td>\n",
              "      <td>explain how to implement dropout in   dropout...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>Describe the process of building a new sectio...</td>\n",
              "      <td>91.75</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>describe the process of building image, okay ...</td>\n",
              "      <td>describe the process of building image   the ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32456cbf-ca09-4702-86f9-925bb345557e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32456cbf-ca09-4702-86f9-925bb345557e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32456cbf-ca09-4702-86f9-925bb345557e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0dcd6bcd-3b79-4d56-992f-2f172436ed8a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0dcd6bcd-3b79-4d56-992f-2f172436ed8a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0dcd6bcd-3b79-4d56-992f-2f172436ed8a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_merge",
              "summary": "{\n  \"name\": \"df_merge\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_auto\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can you describe your experience with transfer learning and TensorFlow, how do you benefit from it? Ah, okay. About transfer learning is actually, we use existing trained model from TensorFlow, for example, like VGC16, VGC19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is, that's already a trained model with exceptionally high accuracy, high performance. Yeah, even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGC19, VGC16, EfficientNet, it will help to improve our models, comparing to the one if you use a traditional CNN model. So, yeah, CNN model with the convolutional 2D, yeah, max pooling, and yeah, it's quite good, actually, to use transfer learning. It really helps with our model performance, to improve our model performance.\",\n          \" Describe the process of building a new section, okay. The CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class, okay? And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset, right? Yeah, of course, we can use another set, which is the test dataset, yeah. But, yeah, okay, the next one, we can just, maybe we need to implement also the image augmentation, yeah, data image augmentation to make our dataset more veritative, right? For example, like, we can rotate, we can zoom it, we can crop it, yeah. And, yeah, the last thing, yeah, of course, we can build our chain model with the Convolutional 2D, specify the filters, the kernel size, the LRM definition, of course, the input shape for the first layer, and then we can apply the Max Pooling 2D, yeah, and the next layer, we can just use Convolutional 2D, Max Pooling, and whatever it is. And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dash layer, right, for the output, the last output layer.\",\n          \" Describe a core model you've built and the steps you took to ensure accuracy and efficiency. Complex transfer model you've built to ensure accuracy. Okay, I will take one of my previous projects that I used. I also used Keras transfer model. It is about celiac disease prediction. This is also I used the research project for my undergraduate thesis, for my scripts. I used this model. It's quite challenging. Even though it's achieved high accuracy with some dense layer, with some drawout layer. And trial and error also with the callback function with the neurons. But the problem is the dataset is not balanced. So it has the imbalanced class dataset. The approach that I used is just to use the technique called smooth and synthetic oversampling technique with edited nearest neighbor. Which is basically just oversampling and undersampling the dataset. It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stt_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9267146270562436,\n        \"min\": 90.15,\n        \"max\": 92.71,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          92.71,\n          91.75,\n          91.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_y\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file_y\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Can describe your experience with transfer learning in tensorflow? How do you benefit from the projects? ah, ok, uh About transfer learning is actually uh we use existing train model from tensorflow for example uh like VGG16, VGG19 right uh especially uh for for some cases uh that we need to use deep learning using Keras applications uh for example like image classification, we can use transfer learning uh models which is that's already uh trained model with uh exceptionally uh high accuracy high performance uh yeah even though it's trained with different uh datasets but it it really helps uh to improve uh our uh model performance uh, model accuracy, model loss uh and yeah uh for example like mobile net, VGG19, VGG16 ya, efficient net uh. It will help to improve our our models uh comparing to the one if you use a traditional uh CNN model yeah CNN model uh with the convolutional uh 2d yeah max pooling and yeah it's it's quite good actually to use uh transfer learning it really helps uh with our model performance to improve our model performance.\",\n          \" describe the process of building image, okay uh the cnn one, right so, uh at the first time uh of course uh, we need to make sure uh there are split uh the the image folder split for for uh each uh class okay, uh and then we can use uh Keras preprocessing if I'm not mistaken uh image dataset from directory to split the training and the validation uh dataset right, uh yeah of course we can use another another set which is the test dataset, yeah uh but yeah, okay the next one uh we can, uh yeah we can just uh maybe we we need to implement also the image augmentation yeah, data uh data image data augmentation to uh to make our dataset more uh variative, right for example like we can rotate we can zoom it, we can uh crop it yeah, uh and yeah the last thing, yeah of course we we can build our gen model with the convolutional 2d uh specify the filters, the kernel size, the LRF division of course the input the input shape for the first layer and then we can apply the max pooling 2d uh yeah, and the next layer we can just use convolutional 2D, max pooling and uh whatever it is uh and after that uh we apply the flatten layer uh and dropout layer if you want and the last thing uh don't forget to use the dense layer, right for the output, like uh the last.\",\n          \" Describe a complex tensorflow model you've built and the steps you took to ensure its accuracy and efficiency hm, uh complex tensorflow model you've built and steps you took to ensure its accuracy hm, ok, uh i will take one of my uh previous project that uh i use, yeah i also use keras tensorflow model uh its it is uh about uh celiac disease prediction ya uh yeah, this is also i use uh the research project for my uh undergraduate thesis yeah for my skripsi and uh i use this model, it's quite challenging uh even though it's uh achieved high accuracy yeah with with some dense layer yeah, with some uh dropout layer and trial and error with the uh also with the uh callback function yeah uh with the neurons but the problem is the dataset is not uh balance yeah so it has the imbalanced uh class datasets yeah and uh the the approach that i use is to yeah just to use uh the technique called smote and yeah synthetic oversampling technique with edited nearest neighbor. yeah which is, uh Basically it's just oversampling and undersampling uh the datasets. uh It helps with the accuracy.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" can describe your experience with transfer learning in tensorflow how do you benefit from the projects    about transfer learning is actually  we use existing train model from tensorflow for example  like vgg16 vgg19 right  especially  for for some cases  that we need to use deep learning using keras applications  for example like image classification we can use transfer learning  models which is thats already  trained model with  exceptionally  high accuracy high performance   even though its trained with different  datasets but it it really helps  to improve  our  model performance  model accuracy model loss  and   for example like mobile net vgg19 vgg16  efficient net  it will help to improve our our models  comparing to the one if you use a traditional  cnn model  cnn model  with the convolutional  2d  max pooling and  its its quite good actually to use  transfer learning it really helps  with our model performance to improve our model performance\",\n          \" describe the process of building image   the cnn one right so  at the first time  of course  we need to make sure  there are split  the the image folder split for for  each  class   and then we can use  keras preprocessing if im not mistaken  image dataset from directory to split the training and the validation  dataset right   of course we can use another another set which is the test dataset   but   the next one  we can   we can just  maybe we we need to implement also the image augmentation  data  data image data augmentation to  to make our dataset more  variative right for example like we can rotate we can zoom it we can  crop it   and  the last thing  of course we we can build our gen model with the convolutional 2d  specify the filters the kernel size the lrf division of course the input the input shape for the first layer and then we can apply the max pooling 2d   and the next layer we can just use convolutional 2d max pooling and  whatever it is  and after that  we apply the flatten layer  and dropout layer if you want and the last thing  dont forget to use the dense layer right for the output like  the last\",\n          \" describe a complex tensorflow model youve built and the steps you took to ensure its accuracy and efficiency   complex tensorflow model youve built and steps you took to ensure its accuracy    i will take one of my  previous project that  i use  i also use keras tensorflow model  its it is  about  celiac disease prediction    this is also i use  the research project for my  undergraduate thesis  for my skripsi and  i use this model its quite challenging  even though its  achieved high accuracy  with with some dense layer  with some  dropout layer and trial and error with the  also with the  callback function   with the neurons but the problem is the dataset is not  balance  so it has the imbalanced  class datasets  and  the the approach that i use is to  just to use  the technique called smote and  synthetic oversampling technique with edited nearest neighbor  which is  basically its just oversampling and undersampling  the datasets  it helps with the accuracy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_merge.to_csv(\"dataset_final_for_training.csv\", index=False)\n",
        "print(\"Berhasil membuat dataset_final_for_training.csv\")"
      ],
      "metadata": {
        "id": "cOPk35zBNDZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c71384-e0ee-44ed-bd43-2e727fe56e67"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Berhasil membuat dataset_final_for_training.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WER"
      ],
      "metadata": {
        "id": "6fEqXi_Bo8hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset_final_for_training.csv\")\n",
        "\n",
        "wers = []\n",
        "cers = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    ref = row[\"transcript_manual\"]   # ground truth\n",
        "    hyp = row[\"transcript_auto\"]     # hasil transkrip menggunakan whisper\n",
        "\n",
        "    wers.append(wer(ref, hyp))\n",
        "    cers.append(cer(ref, hyp))\n",
        "\n",
        "df[\"WER\"] = wers\n",
        "df[\"CER\"] = cers\n",
        "\n",
        "df.to_csv(\"dataset_with_metrics.csv\", index=False)\n",
        "\n",
        "print(df[[\"index\",\"WER\",\"CER\"]])\n",
        "print(\"\\nEvaluasi selesai, hasil disimpan ke dataset_with_metrics.csv\")"
      ],
      "metadata": {
        "id": "ElZfTXdsNXR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ae65f9-7722-457b-f4db-04f9e45c2ec6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index       WER       CER\n",
            "0      1  0.401235  0.188506\n",
            "1      2  0.435484  0.162264\n",
            "2      3  0.492537  0.248624\n",
            "3      4  0.348936  0.176814\n",
            "4      5  0.438735  0.200000\n",
            "\n",
            "Evaluasi selesai, hasil disimpan ke dataset_with_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rata-rata WER:\", df[\"WER\"].mean())\n",
        "print(\"Rata-rata CER:\", df[\"CER\"].mean())"
      ],
      "metadata": {
        "id": "liDxVEquNbMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9396e057-8b62-46c5-eee5-8a137700ccab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rata-rata WER: 0.4233854200760382\n",
            "Rata-rata CER: 0.19524155259144785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(\"WER\", ascending=False).head()"
      ],
      "metadata": {
        "id": "SGu89LRDNcgt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "da2409af-8457-4c94-de14-d22be3aebee6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index                                         question_x  \\\n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "0      1  Can you share any specific challenges you face...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "\n",
              "         audio_file_x                                    transcript_auto  \\\n",
              "2  audios/audio_3.wav   Describe a core model you've built and the st...   \n",
              "4  audios/audio_5.wav   Describe the process of building a new sectio...   \n",
              "1  audios/audio_2.wav   Can you describe your experience with transfe...   \n",
              "0  audios/audio_1.wav   Can you share any specific challenges faced w...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in test serv...   \n",
              "\n",
              "   stt_confidence                                         question_y  \\\n",
              "2           91.45  Describe a complex TensorFlow model you have b...   \n",
              "4           91.75  Describe the process of building a convolution...   \n",
              "1           92.71  Can you describe your experience with transfer...   \n",
              "0           91.19  Can you share any specific challenges you face...   \n",
              "3           90.15  Explain how to implement dropout in a TensorFl...   \n",
              "\n",
              "         audio_file_y                                  transcript_manual  \\\n",
              "2  audios/audio_3.wav   Describe a complex tensorflow model you've bu...   \n",
              "4  audios/audio_5.wav   describe the process of building image, okay ...   \n",
              "1  audios/audio_2.wav   Can describe your experience with transfer le...   \n",
              "0  audios/audio_1.wav   Can share any specific challenges you faced w...   \n",
              "3  audios/audio_4.wav   Explain how to implement dropout in ah ok dro...   \n",
              "\n",
              "                           transcript_manual_cleaned       WER       CER  \n",
              "2   describe a complex tensorflow model youve bui...  0.492537  0.248624  \n",
              "4   describe the process of building image   the ...  0.438735  0.200000  \n",
              "1   can describe your experience with transfer le...  0.435484  0.162264  \n",
              "0   can share any specific challenges you faced w...  0.401235  0.188506  \n",
              "3   explain how to implement dropout in   dropout...  0.348936  0.176814  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a82c16b-05c3-468b-a22d-6e42ad92aa43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question_x</th>\n",
              "      <th>audio_file_x</th>\n",
              "      <th>transcript_auto</th>\n",
              "      <th>stt_confidence</th>\n",
              "      <th>question_y</th>\n",
              "      <th>audio_file_y</th>\n",
              "      <th>transcript_manual</th>\n",
              "      <th>transcript_manual_cleaned</th>\n",
              "      <th>WER</th>\n",
              "      <th>CER</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a core model you've built and the st...</td>\n",
              "      <td>91.45</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>Describe a complex tensorflow model you've bu...</td>\n",
              "      <td>describe a complex tensorflow model youve bui...</td>\n",
              "      <td>0.492537</td>\n",
              "      <td>0.248624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>Describe the process of building a new sectio...</td>\n",
              "      <td>91.75</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>describe the process of building image, okay ...</td>\n",
              "      <td>describe the process of building image   the ...</td>\n",
              "      <td>0.438735</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can you describe your experience with transfe...</td>\n",
              "      <td>92.71</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>Can describe your experience with transfer le...</td>\n",
              "      <td>can describe your experience with transfer le...</td>\n",
              "      <td>0.435484</td>\n",
              "      <td>0.162264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can you share any specific challenges faced w...</td>\n",
              "      <td>91.19</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>Can share any specific challenges you faced w...</td>\n",
              "      <td>can share any specific challenges you faced w...</td>\n",
              "      <td>0.401235</td>\n",
              "      <td>0.188506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in test serv...</td>\n",
              "      <td>90.15</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>Explain how to implement dropout in ah ok dro...</td>\n",
              "      <td>explain how to implement dropout in   dropout...</td>\n",
              "      <td>0.348936</td>\n",
              "      <td>0.176814</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a82c16b-05c3-468b-a22d-6e42ad92aa43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a82c16b-05c3-468b-a22d-6e42ad92aa43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a82c16b-05c3-468b-a22d-6e42ad92aa43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3f15be77-7a46-408c-b866-79fabe75ca49\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3f15be77-7a46-408c-b866-79fabe75ca49')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3f15be77-7a46-408c-b866-79fabe75ca49 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Explain how to implement dropout in a TensorFlow model and the effect it has on training.\",\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_5.wav\",\n          \"audios/audio_4.wav\",\n          \"audios/audio_2.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_auto\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" Describe the process of building a new section, okay. The CNN one, right? So, at the first time, of course, we need to make sure the image folder is split for each class, okay? And then we can use Keras preprocessing, if I'm not mistaken, image dataset from directory to split the training and the validation dataset, right? Yeah, of course, we can use another set, which is the test dataset, yeah. But, yeah, okay, the next one, we can just, maybe we need to implement also the image augmentation, yeah, data image augmentation to make our dataset more veritative, right? For example, like, we can rotate, we can zoom it, we can crop it, yeah. And, yeah, the last thing, yeah, of course, we can build our chain model with the Convolutional 2D, specify the filters, the kernel size, the LRM definition, of course, the input shape for the first layer, and then we can apply the Max Pooling 2D, yeah, and the next layer, we can just use Convolutional 2D, Max Pooling, and whatever it is. And after that, we apply the flatten layer and dropout layer, if you want. And the last thing, don't forget to use the dash layer, right, for the output, the last output layer.\",\n          \" Explain how to implement dropout in test server model and test on training. Previously, I also have implemented the dropout layer also in the project function within this certifications and we can just add the dropout layer, for example, if I'm not mistaken, I have used this dropout layer in the one that the case is image classifications, yeah, a German traffic something, if I'm not wrong, I have used this dropout layer in the, not in the last, in the middle of the layer, so there's a flattened layer, right, not flattened, the convolutional layer and the flattened layer and I use that dropout layer which is I use with the rate of 0.2 or 0.5, if I'm not wrong, and then the dense layer and the last, the output layer, right, the effect is it will really help to improve our accuracy and lower our validation loss by turning off some of the previous layer, yeah, for example, like we have a dense layer before and the next layer, we implement the dropout layer with the rate of 0.5 and it will turn off randomly each epoch for the previous dense layer. Okay.\",\n          \" Can you describe your experience with transfer learning and TensorFlow, how do you benefit from it? Ah, okay. About transfer learning is actually, we use existing trained model from TensorFlow, for example, like VGC16, VGC19, right? Especially for some cases that we need to use deep learning using Keras applications, for example, like image classification, we can use transfer learning models, which is, that's already a trained model with exceptionally high accuracy, high performance. Yeah, even though it's trained with different data sets, but it really helps to improve our model performance, model accuracy, model loss. For example, like MobileNet, VGC19, VGC16, EfficientNet, it will help to improve our models, comparing to the one if you use a traditional CNN model. So, yeah, CNN model with the convolutional 2D, yeah, max pooling, and yeah, it's quite good, actually, to use transfer learning. It really helps with our model performance, to improve our model performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stt_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9267146270562436,\n        \"min\": 90.15,\n        \"max\": 92.71,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          91.75,\n          90.15,\n          92.71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_y\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Explain how to implement dropout in a TensorFlow model and the effect it has on training.\",\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file_y\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_5.wav\",\n          \"audios/audio_4.wav\",\n          \"audios/audio_2.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" describe the process of building image, okay uh the cnn one, right so, uh at the first time uh of course uh, we need to make sure uh there are split uh the the image folder split for for uh each uh class okay, uh and then we can use uh Keras preprocessing if I'm not mistaken uh image dataset from directory to split the training and the validation uh dataset right, uh yeah of course we can use another another set which is the test dataset, yeah uh but yeah, okay the next one uh we can, uh yeah we can just uh maybe we we need to implement also the image augmentation yeah, data uh data image data augmentation to uh to make our dataset more uh variative, right for example like we can rotate we can zoom it, we can uh crop it yeah, uh and yeah the last thing, yeah of course we we can build our gen model with the convolutional 2d uh specify the filters, the kernel size, the LRF division of course the input the input shape for the first layer and then we can apply the max pooling 2d uh yeah, and the next layer we can just use convolutional 2D, max pooling and uh whatever it is uh and after that uh we apply the flatten layer uh and dropout layer if you want and the last thing uh don't forget to use the dense layer, right for the output, like uh the last.\",\n          \" Explain how to implement dropout in ah ok dropout in tensorflow model and it test on training. hm Previously, uh I also have uh implement the dropout layer yeah also in the project submission uh within this certifications. and We can just add the dropout layer. uh For example, uh if I'm not mistaken, I have used this dropout layer in the the one that the case is hm yeah image classifications yeah. German traffic, something if I'm not wrong. I have used this dropout layer uh in in in the not in the last uh in the middle in the middle of the layer. uh So there there's a flattened layer right not flattened, the convolutional layer and the flattened layer. uh and I use that dropout layer, uh uh which is uh I used with the rate of 0.2 or 0.5 if if I'm not wrong. uh And then the dense layer and the the last, the output layer right. uh The effect is it will really helps to hm improve our accuracy and lower our validation loss uh by by turning off some of the previous layer. yeah For example like, we have dense layer 64 and the next layer, uh we implement the dropout uh layer with the rate of uh o point five and it will uh turn off randomly each epoch uh for of the previous dense layer uh\",\n          \" Can describe your experience with transfer learning in tensorflow? How do you benefit from the projects? ah, ok, uh About transfer learning is actually uh we use existing train model from tensorflow for example uh like VGG16, VGG19 right uh especially uh for for some cases uh that we need to use deep learning using Keras applications uh for example like image classification, we can use transfer learning uh models which is that's already uh trained model with uh exceptionally uh high accuracy high performance uh yeah even though it's trained with different uh datasets but it it really helps uh to improve uh our uh model performance uh, model accuracy, model loss uh and yeah uh for example like mobile net, VGG19, VGG16 ya, efficient net uh. It will help to improve our our models uh comparing to the one if you use a traditional uh CNN model yeah CNN model uh with the convolutional uh 2d yeah max pooling and yeah it's it's quite good actually to use uh transfer learning it really helps uh with our model performance to improve our model performance.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_manual_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" describe the process of building image   the cnn one right so  at the first time  of course  we need to make sure  there are split  the the image folder split for for  each  class   and then we can use  keras preprocessing if im not mistaken  image dataset from directory to split the training and the validation  dataset right   of course we can use another another set which is the test dataset   but   the next one  we can   we can just  maybe we we need to implement also the image augmentation  data  data image data augmentation to  to make our dataset more  variative right for example like we can rotate we can zoom it we can  crop it   and  the last thing  of course we we can build our gen model with the convolutional 2d  specify the filters the kernel size the lrf division of course the input the input shape for the first layer and then we can apply the max pooling 2d   and the next layer we can just use convolutional 2d max pooling and  whatever it is  and after that  we apply the flatten layer  and dropout layer if you want and the last thing  dont forget to use the dense layer right for the output like  the last\",\n          \" explain how to implement dropout in   dropout in tensorflow model and it test on training  previously  i also have  implement the dropout layer  also in the project submission  within this certifications and we can just add the dropout layer  for example  if im not mistaken i have used this dropout layer in the the one that the case is   image classifications  german traffic something if im not wrong i have used this dropout layer  in in in the not in the last  in the middle in the middle of the layer  so there theres a flattened layer right not flattened the convolutional layer and the flattened layer  and i use that dropout layer   which is  i used with the rate of 02 or 05 if if im not wrong  and then the dense layer and the the last the output layer right  the effect is it will really helps to  improve our accuracy and lower our validation loss  by by turning off some of the previous layer  for example like we have dense layer 64 and the next layer  we implement the dropout  layer with the rate of  o point five and it will  turn off randomly each epoch  for of the previous dense layer \",\n          \" can describe your experience with transfer learning in tensorflow how do you benefit from the projects    about transfer learning is actually  we use existing train model from tensorflow for example  like vgg16 vgg19 right  especially  for for some cases  that we need to use deep learning using keras applications  for example like image classification we can use transfer learning  models which is thats already  trained model with  exceptionally  high accuracy high performance   even though its trained with different  datasets but it it really helps  to improve  our  model performance  model accuracy model loss  and   for example like mobile net vgg19 vgg16  efficient net  it will help to improve our our models  comparing to the one if you use a traditional  cnn model  cnn model  with the convolutional  2d  max pooling and  its its quite good actually to use  transfer learning it really helps  with our model performance to improve our model performance\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05290870200232052,\n        \"min\": 0.34893617021276596,\n        \"max\": 0.4925373134328358,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.43873517786561267,\n          0.34893617021276596,\n          0.43548387096774194\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.032957424050100007,\n        \"min\": 0.16226415094339622,\n        \"max\": 0.2486238532110092,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2,\n          0.176814011676397,\n          0.16226415094339622\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WER/CER Final Evaluation (Normalisasi + Hapus Filler + Mapping Pengucapan)\n"
      ],
      "metadata": {
        "id": "MindML9hXdCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"dataset_final_for_training.csv\")\n",
        "\n",
        "# Fungsi preprocessing lengkap (text mapping)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    replacements = {\n",
        "        \"vgc16\": \"vgg16\",\n",
        "        \"vgc19\": \"vgg19\",\n",
        "        \"mobilenet\": \"mobile net\",\n",
        "        \"drawout\": \"dropout\",\n",
        "        \"draw out\": \"dropout\",\n",
        "        \"smooth\": \"smote\",\n",
        "        \"core model\": \"complex tensorflow model\",\n",
        "        \"chain model\": \"cnn model\",\n",
        "        \"dash layer\": \"dense layer\",\n",
        "        \"dense c4\": \"dense layer 64\",\n",
        "        \"flat layer\": \"flatten layer\",\n",
        "        \"flat ten\": \"flatten\",\n",
        "        \"per processing\": \"preprocessing\",\n",
        "        \"pre processing\": \"preprocessing\",\n",
        "        \"the set\": \"dataset\",\n",
        "        \"data set\": \"dataset\",\n",
        "        \"set aside\": \"dataset\",\n",
        "        \"the data\": \"dataset\",\n",
        "        \"conversion to be\": \"convolutional 2d\",\n",
        "        \"convolution to d\": \"convolutional 2d\",\n",
        "        \"next pulling\": \"max pooling\",\n",
        "        \"max pooling to d\": \"max pooling 2d\",\n",
        "        \"celia\": \"celiac\",\n",
        "        \"silic\": \"celiac\",\n",
        "        \"celiax\": \"celiac\",\n",
        "        \"sealick\": \"celiac\",\n",
        "        \"scripcy\": \"skripsi\",\n",
        "\n",
        "        # Tambahan untuk video 3\n",
        "        \"callback\": \"callback function\",\n",
        "        \"scripts\": \"skripsi\",\n",
        "        \"script\": \"skripsi\",\n",
        "        \"transfer model\": \"tensorflow model\",\n",
        "        \"imbalanced class dataset\": \"imbalanced dataset\",\n",
        "        \"core model\": \"complex tensorflow model\",\n",
        "        \"drawout\": \"dropout\",\n",
        "        \"smooth\": \"smote\",\n",
        "        \"transfer model\": \"tensorflow model\",\n",
        "        \"also used\": \"also use\",\n",
        "        \"used\": \"use\",\n",
        "        \"achieved\": \"achieve\",\n",
        "        \"trial and error also with\": \"trial and error with\",\n",
        "        \"just to use the technique called\": \"just to use the technique\",\n",
        "        \"okay\": \"\",\n",
        "        \"accuracy and efficiency complex tensorflow model you ve built\": \"accuracy and efficiency\",\n",
        "        \"ensure accuracy and efficiency complex tensorflow model\": \"ensure accuracy and efficiency\",\n",
        "        \"tensorflow model you ve built to ensure accuracy\": \"tensorflow model to ensure accuracy\",\n",
        "        \"i used\": \"i use\",\n",
        "        \"i also used\": \"i also use\",\n",
        "        \"trial and error also with\": \"trial and error with\",\n",
        "        \"just to use the technique called\": \"just to use the technique\",\n",
        "        \"okay\": \"\",\n",
        "        \"datasetset\": \"dataset\",\n",
        "        \"datasets\": \"dataset\",\n",
        "        \"its it is\": \"it is\",\n",
        "        \"with the also with the\": \"with the\",\n",
        "        \"projects that i used\": \"projects that i use\",\n",
        "        \"the steps you took to ensure\": \"to ensure\",\n",
        "        \"its accuracy and efficiency\": \"accuracy and efficiency\",\n",
        "\n",
        "        # Tambahan untuk video 5\n",
        "        \"dash\": \"dense\",\n",
        "        \"dash layer\": \"dense layer\",\n",
        "        \"convolutional to d\": \"convolutional 2d\",\n",
        "        \"max pooling to d\": \"max pooling 2d\",\n",
        "        \"flatten layer\": \"flatten\",\n",
        "        \"keras preprocessing\": \"image data preprocessing\",\n",
        "        \"image of condition\": \"image augmentation\",\n",
        "        \"veritative\": \"variative\",\n",
        "        \"l i m definition\": \"activation function\",\n",
        "        \"new section\": \"image classification\",\n",
        "        \"dash\": \"dense\",\n",
        "        \"dash layer\": \"dense layer\",\n",
        "        \"convolutional2d\": \"convolutional 2d\",\n",
        "        \"maxpooling\": \"max pooling\",\n",
        "        \"veritative\": \"variative\",\n",
        "        \"l i m definition\": \"activation function\"}\n",
        "\n",
        "    for wrong, correct in replacements.items():\n",
        "        text = text.replace(wrong, correct)\n",
        "\n",
        "    fillers = [\n",
        "        r\"\\buh\\b\", r\"\\bum\\b\", r\"\\bah\\b\", r\"\\boh\\b\", r\"\\beh\\b\",\n",
        "        r\"\\byeah\\b\", r\"\\bokay\\b\", r\"\\bok\\b\", r\"\\bhm\\b\", r\"\\bem\\b\"]\n",
        "\n",
        "    for f in fillers:\n",
        "        text = re.sub(f, \" \", text)\n",
        "\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "sHcJeKWgQHTG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terapkan ke GT dan hypothesis\n",
        "df[\"ref_no_filler\"] = df[\"transcript_manual\"].apply(preprocess_text)\n",
        "df[\"hyp_no_filler\"] = df[\"transcript_auto\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "0rUs1pSrbmB4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hitung metrik akhir\n",
        "df[\"WER_final\"] = df.apply(lambda row: wer(row[\"ref_no_filler\"], row[\"hyp_no_filler\"]), axis=1)\n",
        "df[\"CER_final\"] = df.apply(lambda row: cer(row[\"ref_no_filler\"], row[\"hyp_no_filler\"]), axis=1)"
      ],
      "metadata": {
        "id": "NglxmKUabpKn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_wer = wer(\" \".join(df.ref_no_filler), \" \".join(df.hyp_no_filler))\n",
        "global_cer = cer(\" \".join(df.ref_no_filler), \" \".join(df.hyp_no_filler))"
      ],
      "metadata": {
        "id": "opDI5cTIX0B_",
        "collapsed": true
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Evaluasi per Audio\n",
        "print(\"\\nHasil per-audio:\")\n",
        "print(df[[\"index\", \"WER_final\", \"CER_final\"]].rename(columns={\"WER_final\": \"WER\", \"CER_final\": \"CER\"}))"
      ],
      "metadata": {
        "id": "gIGBiI2v7stG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc04691d-7e76-4c99-ebe3-985680b8573c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hasil per-audio:\n",
            "   index       WER       CER\n",
            "0      1  0.089552  0.067194\n",
            "1      2  0.084416  0.039957\n",
            "2      3  0.113924  0.079153\n",
            "3      4  0.083333  0.063628\n",
            "4      5  0.091346  0.082166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Global WER:\", global_wer)\n",
        "print(\"Global CER:\", global_cer)\n",
        "print(\"Akurasi STT (WER):\", 1 - global_wer)\n",
        "print(\"Akurasi STT (CER):\", 1 - global_cer)"
      ],
      "metadata": {
        "id": "vKJi_ldaewls",
        "outputId": "62b653a5-9e91-4330-95d2-e2c8ab1939a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global WER: 0.09207459207459208\n",
            "Global CER: 0.06666666666666667\n",
            "Akurasi STT (WER): 0.9079254079254079\n",
            "Akurasi STT (CER): 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil akhir\n",
        "df.to_csv(\"final_transcript_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "gwtfZTGvb2CN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**WAV2VEC2**"
      ],
      "metadata": {
        "id": "nHmPxZcwct7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Wav2Vec2 model...\")\n",
        "model_name = \"facebook/wav2vec2-large-robust-ft-libri-960h\"\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "b1807044904a41f2abc3cf72f0a5fe3d",
            "d99920002ca6497686f4e9c747f83b37",
            "e12acd48fdba4ca8a5a5ca9b641d7655",
            "7d33a7fbf4af48909bdfdf45fd6f4845",
            "a0344a7cbcc74d1182522337ea5ad7e0",
            "372f014095ea4916aea4199b28c9a68c",
            "2e9e4506c27b47c4a263274a50982b53",
            "1844695c258a4860a031ba7d2faa5b5c",
            "6656d90fbc784fc9a85f1c82644790ea",
            "76567c377cb649c9a3fb7c0a98a3edbd",
            "b094df04850c4bb3bda621b574f9d1fe",
            "1ae46c81d9e7426da1987460d9759d7a",
            "463ea277f57b403ca32849824e531995",
            "00e27d6e3b264eca8581e2d5e0f8b199",
            "63a8bfbd69c748f69a5ea55d819f1350",
            "bac163bad1f5411f99ee5d1146abac2e",
            "352ba84b623346768ea2f7beec6f8de9",
            "ddc055ee5c3e46e39724103e2808e8b1",
            "daf4d51e45514ffebfc24e03a30cb7b4",
            "0ca117f287b24d4c828916e6dd001214",
            "db37436578394ddaa9adc8c78114d944",
            "db081be90a814ba79b805059a51a0b90",
            "ea683f1cb5ef450da357f91137fd287d",
            "c1cef9eec5904f559b8db1e2142ccfab",
            "42cdb2dcbf804d91b30629772b481971",
            "01a80e86777a4e0eb5ec9365e6ddea6d",
            "e638b519403c4bfb837d335b5ec86374",
            "c20eb1d238ef493794de709dcccff43c",
            "7d45ab9f07d94a6ca50e881c44d0a669",
            "35ae89b38f044213b590d13435d89994",
            "c793fdc26ec7423e95173276ba482bd4",
            "2eefc3e5df4b4ad8812f52e030fc7246",
            "9f0de277452542bda5d92eb1bce300c8",
            "0f5710e0dc0c4ce7809d8269f092d18c",
            "08fc3d6838244b5ba6ec0b5554ce1f3e",
            "d4441be2fb504a34ba0390568ec7f59a",
            "d9e294727e8643f4a5693b4954a74d59",
            "4fddd4fe7d4c4050a2b7f686c2166909",
            "1e355d5cc90f4653892a66aa74be9993",
            "e26487ebdde346a6a9fb483ea78966c0",
            "7ed1a7ae0e3f49418f9c5d7b25550fe1",
            "f61a471a0b9249f0a886f5c05486bab6",
            "e2ca43ad9a074272af417dcb3e0103fd",
            "b24cb278180d4272b2bdbbe4e8a510d6",
            "7130950fea8e47158b63b9f30c35197c",
            "e3f7c54bca72477992086e029ce36ae5",
            "ba6f41c203a346b1945aeedc909059bc",
            "76be6ac62ec845f9a7b8c72b75ff202b",
            "f52b232b84324fb3a28548b67f87b6a1",
            "499123e471284a618e259be5f7de12be",
            "07783045f4ca46348c5f43b4f52ee8d9",
            "6be9a55668854fc19739011f27f761cb",
            "d7cba12b395e42bea661f583043fa5a1",
            "281d716053e8421c914083e2193509f9",
            "18ba1e8e9d324ca1a3760850ba64e23c",
            "9e079d6875764a0c81342575012b1622",
            "e5d6411ed78546f0b10aad0ce1c3238b",
            "e031c8740bee4a2a99ac628c98fd7148",
            "a8ae2cdd61554d0fa70c346563c46e51",
            "c50e6e43e48447bf90712cfe46641ad1",
            "55b9b24bb6b746509aaa03e7f6ea4943",
            "cef1b99d22d845f784cfbc3b2ee1abb7",
            "2bfad4c2f37a48ea8ffa0fe0acc32aa7",
            "043d5ebecea14b2a90c75d82cf77a4f6",
            "faec63d08ff84670941534a3c1cb228f",
            "96f46443b1534c05ac53c0de7e8a896a"
          ]
        },
        "id": "jaTbxLgsJAWu",
        "outputId": "ce76cdf3-c03d-4a0d-9e86-c987aa8fded1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Wav2Vec2 model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1807044904a41f2abc3cf72f0a5fe3d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ae46c81d9e7426da1987460d9759d7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea683f1cb5ef450da357f91137fd287d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f5710e0dc0c4ce7809d8269f092d18c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7130950fea8e47158b63b9f30c35197c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e079d6875764a0c81342575012b1622"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ground truth\n",
        "df_gt = pd.read_csv(\"ground_truth_cleaned.csv\")"
      ],
      "metadata": {
        "id": "LP2y0ki5ns1b"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transkripsi semua audio\n",
        "transcripts = []\n",
        "for idx, row in df_gt.iterrows():\n",
        "    audio_path = row['audio_file']\n",
        "    print(f\"Transcribing: {audio_path}\")\n",
        "    try:\n",
        "        audio, sr = torchaudio.load(audio_path)\n",
        "        if sr != 16000:\n",
        "            audio = torchaudio.transforms.Resample(sr, 16000)(audio)\n",
        "        if audio.shape[0] > 1:\n",
        "            audio = torch.mean(audio, dim=0, keepdim=True)\n",
        "        input_values = processor(audio.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "        input_values = input_values.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_values).logits\n",
        "        pred_ids = torch.argmax(logits, dim=-1)\n",
        "        text = processor.batch_decode(pred_ids)[0]\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        text = \"\"\n",
        "    transcripts.append(text)\n",
        "\n",
        "df_gt['transcript_wav2vec2'] = transcripts\n",
        "df_gt.to_csv(\"dataset_transcript_wav2vec2.csv\", index=False)\n",
        "print(\"dataset_transcript_wav2vec2.csv dibuat!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOA8DyX3JM1H",
        "outputId": "43d23d1f-70b7-43d0-c60d-810fc460e239"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing: audios/audio_1.wav\n",
            "Transcribing: audios/audio_2.wav\n",
            "Transcribing: audios/audio_3.wav\n",
            "Transcribing: audios/audio_4.wav\n",
            "Transcribing: audios/audio_5.wav\n",
            "dataset_transcript_wav2vec2.csv dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menampilkan hasil transcript wav2vec2\n",
        "display(df_gt[['index', 'question', 'audio_file', 'transcript_wav2vec2']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BNmTvLxUhur8",
        "outputId": "34d12b14-df15-4281-addc-6c15f73eb9fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   index                                           question  \\\n",
              "0      1  Can you share any specific challenges you face...   \n",
              "1      2  Can you describe your experience with transfer...   \n",
              "2      3  Describe a complex TensorFlow model you have b...   \n",
              "3      4  Explain how to implement dropout in a TensorFl...   \n",
              "4      5  Describe the process of building a convolution...   \n",
              "\n",
              "           audio_file                                transcript_wav2vec2  \n",
              "0  audios/audio_1.wav  CASAISPECFECTORIOS FACE WER WOKING ONSTION O L...  \n",
              "1  audios/audio_2.wav  CAN DESCRIBE YOUR SPIRITS WE TRANSFOR ME N TAN...  \n",
              "2  audios/audio_3.wav  ESCRIPOFOR MODEL YOU FIL AN DESTEPS UTOENSION ...  \n",
              "3  audios/audio_4.wav  WI HOL YOU TO EPEEN DROPOTIN A OE DROW OUT TEI...  \n",
              "4  audios/audio_5.wav  RESCRIBE THE POCESITINFICTION OA N SE AN ANON ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7b6ce59-fb83-4ed3-a8b5-7e8096023039\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>question</th>\n",
              "      <th>audio_file</th>\n",
              "      <th>transcript_wav2vec2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Can you share any specific challenges you face...</td>\n",
              "      <td>audios/audio_1.wav</td>\n",
              "      <td>CASAISPECFECTORIOS FACE WER WOKING ONSTION O L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Can you describe your experience with transfer...</td>\n",
              "      <td>audios/audio_2.wav</td>\n",
              "      <td>CAN DESCRIBE YOUR SPIRITS WE TRANSFOR ME N TAN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Describe a complex TensorFlow model you have b...</td>\n",
              "      <td>audios/audio_3.wav</td>\n",
              "      <td>ESCRIPOFOR MODEL YOU FIL AN DESTEPS UTOENSION ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Explain how to implement dropout in a TensorFl...</td>\n",
              "      <td>audios/audio_4.wav</td>\n",
              "      <td>WI HOL YOU TO EPEEN DROPOTIN A OE DROW OUT TEI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Describe the process of building a convolution...</td>\n",
              "      <td>audios/audio_5.wav</td>\n",
              "      <td>RESCRIBE THE POCESITINFICTION OA N SE AN ANON ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7b6ce59-fb83-4ed3-a8b5-7e8096023039')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a7b6ce59-fb83-4ed3-a8b5-7e8096023039 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a7b6ce59-fb83-4ed3-a8b5-7e8096023039');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-39f9e567-97ea-4713-99d8-74f13b88663b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39f9e567-97ea-4713-99d8-74f13b88663b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-39f9e567-97ea-4713-99d8-74f13b88663b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df_gt[['index', 'question', 'audio_file', 'transcript_wav2vec2']]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Can you describe your experience with transfer learning in TensorFlow? How did it benefit your projects?\",\n          \"Describe the process of building a convolutional neural network (CNN) using TensorFlow for image classification.\",\n          \"Describe a complex TensorFlow model you have built and the steps you took to ensure its accuracy and efficiency.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"audio_file\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"audios/audio_2.wav\",\n          \"audios/audio_5.wav\",\n          \"audios/audio_3.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcript_wav2vec2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"CAN DESCRIBE YOUR SPIRITS WE TRANSFOR ME N TAN FOR HOW WEBINFITFO AH OCAY O ABOUT TRANSRONING IS ACTUALLY A WE USE EXISTING TRAIN MODEL FROM ATENCIOLPL FOR EXAMPLE LIKE A PITIG SIXTEEN FIFTEEN NINETY RIT A ESPECIALLY A FOR FOR SOME CASES A DHAT WE NET TO USE A DILEARNING USING CARROS EPUTATIONS A FOR EXAMPLE LIKE IMAGE CLASIFICATION WE CAN USE TRANSFORMING A MODELS WHICH IS THAT'S ALREADY A TRAIN MODEL WITH EXCEPTIONALLY A HIGH ACCURACY HIGH PERFORMANCE  YA EVEN TOU ITS TRAIN WITH DEFREN A DATE OF SETS BUT ITIT WILL HELPS ATO IMPROVE A OUR A IKA MODEL PERFORMANCE A MODERACURACY MODEL LAWS  AND ONO  FOR EXAMPLE LIKE MOBALNAD A FITIGI NINETEEN FIFTY T SIXTN A A EFICIENT LAT A IT WILL HELP TO IMPROVE OUR OUR MODELS A FCOMPARING TO THE ONE IF WE USE A TRADITIONAL A SIN AND MODEL YA ATIENCIANMODEL W WTHE CONPOLITION O A TUDI IA MAXPULLING AND YA T ITS ITS ER  QUITE GOOD ACTUALLY TO USE TRANSFERNING TROUG THE HELPS  WITH OUR  MODEL PERFORMANCE TO IMPERFOR MTHE PERFORMAN\",\n          \"RESCRIBE THE POCESITINFICTION OA N SE AN ANON ITO  AT THE FIRST TIME A OF COURSE WE NET TO MAXURE A THERE  SPLIT A TE THE IMAGE HOLDERS PLATED FOR FOR A ACH CLASS CA  AND THEN WE CAN USE A CARASPR PROCESSING IAN A SECOND  IMAGE SATASAT FROM THERECTORY TO SPLIT THE TRAINING AND THE FALDATION A LET US HA I WEA OF COURSE WE CAN REDUCE NETHE ANOTHER SET WHICH IS THE PAST BETTER SAT TAR A BUT AMORGETER IN THE NEXT ONE A WE CAN A E ALICAN JUST MAYBE WE WE NT TO INTHEMENT OFSO THE IMAGE OFPINTATION YET DATA E TAT IMIC ATE OPINTATION TOO A TO MAKE OUR LITSAT MORE   FARAT IF I ACAPAL LIT WE CAN TROPAT WE CAN SOIT WE CAN ACROP I TA A AND EA TE LAST THING R O IRST WE CAN DO OUR CEN MODEL WITH THE CONFENSIONAL TO BE A SPECIFY THE FILTERS THE CARNALSISE THE ACTIVITION OF ORC THEIMPUS INPUT SHAPE FOR THE FIRST LAYER AND THEN WE CANLY THE NEX PULLING TO D A  AND NEXT LAYER WE CAN JUST US CONFULSION TO TE NEXPOLING AND A WHATEVER ITIS A AND AFTER THAT WE APPLY THE FLATON MY YAR A AN PROBATLY IF YOU ONE AND THE LAST THING I THONT FORGET TO USE THE DATH LAYER RIT FOR THE OPORT LIKE A DLOSS OT A OT\",\n          \"ESCRIPOFOR MODEL YOU FIL AN DESTEPS UTOENSION US AN EFFICIENCY A COMPLEX SENSIFUL MODEL YO FE NOT INTA ACURACY OKA IWILL TAKE ONE OF MY A RITES PROJECT TAT  I USE YA I ALSO USE CARASN OTENC O FORMODEL A IT'S IT IS A ABOUT CELIAR LISES PREDICTION YA A YA BISIS ALSO I USE A THE RICOS PROJECT FOR MY A UNDERGRAD TIISA OR FOR MY SCRICY AND I USE THIS MODEL IT'S QUITE CHALLENGING A EVEN THOUGH IT'S AACHIVED HIGH ACCURACY NO ITS WITH SOME A DENSALERA WITH SOME A TROWATLAER AND TRIL IN AUE ALSO WITH TE A COLLBET FUNCTION YE  WITH THE NEW ONS BUT THE PROBLEM IS THE DETORSEPT IS NOT A BALANCE YASO AITHAS TE ENVELADS A CLASS DES LETESATYA AND A TTHE THE PROSDET A I USE IS TO GET I JESTTO USE A THE TECHNICE CALL A SMALL AND Y  SYNTHETIC OF ASEMPLING TECHNIC A WI EDITAT NERS NEIGBOR YA WHICH IS A BASICALLY ISJES OWESAMBIG AN UNDERSAMTING A DITED THE DATEOF A  IT PERHAPS WITH ACCURAC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging CSV"
      ],
      "metadata": {
        "id": "qA1-izHXoCMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_gt_full = pd.read_csv(\"ground_truth.csv\")\n",
        "df_w2v2 = pd.read_csv(\"dataset_transcript_wav2vec2.csv\")\n",
        "\n",
        "merged = df_gt_full.merge(df_w2v2[['index', 'transcript_wav2vec2']], on=\"index\")\n",
        "merged.to_csv(\"dataset_final_for_training_w2v2.csv\", index=False)\n",
        "print(\"dataset_final_for_training_w2v2.csv dibuat!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7n7pEu0n_14",
        "outputId": "4e8491da-de2f-4e20-f704-816398c4902a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset_final_for_training_w2v2.csv dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WER"
      ],
      "metadata": {
        "id": "9uzNs8w6iira"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/dataset_final_for_training_w2v2.csv\")\n",
        "\n",
        "wers = []\n",
        "cers = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    ref = row[\"transcript_manual\"]   # ground truth\n",
        "    hyp = row[\"transcript_wav2vec2\"] # hasil transkrip menggunakan whisper\n",
        "\n",
        "    wers.append(wer(ref, hyp))\n",
        "    cers.append(cer(ref, hyp))\n",
        "\n",
        "df[\"WER\"] = wers\n",
        "df[\"CER\"] = cers\n",
        "\n",
        "df.to_csv(\"dataset_with_metrics_w2v2.csv\", index=False)\n",
        "\n",
        "print(df[[\"index\",\"WER\",\"CER\"]])\n",
        "print(\"\\nEvaluasi selesai, hasil disimpan ke dataset_with_metrics_w2v2.csv\")"
      ],
      "metadata": {
        "id": "4-BGT5TCig-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdb19db-09a6-4364-b616-a47768475a84"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   index       WER       CER\n",
            "0      1  1.000000  0.856322\n",
            "1      2  1.000000  0.868868\n",
            "2      3  1.000000  0.849541\n",
            "3      4  0.987234  0.823186\n",
            "4      5  1.000000  0.832411\n",
            "\n",
            "Evaluasi selesai, hasil disimpan ke dataset_with_metrics_w2v2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rata-rata WER:\", df[\"WER\"].mean())\n",
        "print(\"Rata-rata CER:\", df[\"CER\"].mean())"
      ],
      "metadata": {
        "id": "6x0yKZXCjaOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24573d7-79ee-4749-f83f-9a68ead9e97b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rata-rata WER: 0.9974468085106384\n",
            "Rata-rata CER: 0.846065620705942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WER/CER Final Evaluation (Normalisasi + Hapus Filler + Mapping Pengucapan)"
      ],
      "metadata": {
        "id": "p1qN6DNkacyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    replacements = {\n",
        "        \"vgc16\": \"vgg16\", \"vgc19\": \"vgg19\", \"mobilenet\": \"mobile net\", \"drawout\": \"dropout\",\n",
        "        \"draw out\": \"dropout\", \"smooth\": \"smote\", \"core model\": \"complex tensorflow model\",\n",
        "        \"dash layer\": \"dense layer\", \"dense c4\": \"dense layer 64\", \"flat ten\": \"flatten\",\n",
        "        \"per processing\": \"preprocessing\", \"pre processing\": \"preprocessing\", \"data set\": \"dataset\",\n",
        "        \"the set\": \"dataset\", \"set aside\": \"dataset\", \"the data\": \"dataset\",\n",
        "        \"conversion to be\": \"convolutional 2d\", \"convolution to d\": \"convolutional 2d\",\n",
        "        \"next pulling\": \"max pooling\", \"max pooling to d\": \"max pooling 2d\",\n",
        "        \"celia\": \"celiac\", \"silic\": \"celiac\", \"celiax\": \"celiac\", \"sealick\": \"celiac\",\n",
        "        \"scripcy\": \"skripsi\", \"scripts\": \"skripsi\", \"script\": \"skripsi\",\n",
        "        \"callback\": \"callback function\", \"transfer model\": \"tensorflow model\",\n",
        "        \"imbalanced class dataset\": \"imbalanced dataset\", \"also used\": \"also use\", \"used\": \"use\",\n",
        "        \"achieved\": \"achieve\", \"trial and error also with\": \"trial and error with\",\n",
        "        \"just to use the technique called\": \"just to use the technique\",\n",
        "        \"accuracy and efficiency complex tensorflow model you ve built\": \"accuracy and efficiency\",\n",
        "        \"ensure accuracy and efficiency complex tensorflow model\": \"ensure accuracy and efficiency\",\n",
        "        \"tensorflow model you ve built to ensure accuracy\": \"tensorflow model to ensure accuracy\",\n",
        "        \"i used\": \"i use\", \"i also used\": \"i also use\", \"okay\": \"\",\n",
        "        \"datasetset\": \"dataset\", \"datasets\": \"dataset\", \"its it is\": \"it is\",\n",
        "        \"with the also with the\": \"with the\", \"projects that i used\": \"projects that i use\",\n",
        "        \"the steps you took to ensure\": \"to ensure\", \"its accuracy and efficiency\": \"accuracy and efficiency\",\n",
        "        \"dash\": \"dense\", \"convolutional to d\": \"convolutional 2d\",\n",
        "        \"keras preprocessing\": \"image data preprocessing\",\n",
        "        \"image of condition\": \"image augmentation\",\n",
        "        \"veritative\": \"variative\", \"l i m definition\": \"activation function\",\n",
        "        \"new section\": \"image classification\", \"convolutional2d\": \"convolutional 2d\",\n",
        "        \"maxpooling\": \"max pooling\"\n",
        "    }\n",
        "\n",
        "    for wrong, correct in replacements.items():\n",
        "        text = text.replace(wrong, correct)\n",
        "\n",
        "    # Hapus filler words umum\n",
        "    fillers = [\n",
        "        r\"\\buh\\b\", r\"\\bum\\b\", r\"\\bah\\b\", r\"\\boh\\b\", r\"\\beh\\b\",\n",
        "        r\"\\byeah\\b\", r\"\\bokay\\b\", r\"\\bok\\b\", r\"\\bhm\\b\", r\"\\bem\\b\"\n",
        "    ]\n",
        "    for f in fillers:\n",
        "        text = re.sub(f, \" \", text)\n",
        "\n",
        "    # Hapus duplikasi kata, simbol, dan normalisasi whitespace\n",
        "    text = re.sub(r\"\\b(\\w+)( \\1\\b)+\", r\"\\1\", text)  # Menghapus pengulangan kata\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)        # Menghapus alphanumeric\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "ajLthagAJmYh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Terapkan\n",
        "df = pd.read_csv(\"dataset_final_for_training_w2v2.csv\")\n",
        "df[\"ref_clean\"] = df[\"transcript_manual\"].apply(preprocess_text)\n",
        "df[\"hyp_clean\"] = df[\"transcript_wav2vec2\"].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "psCVXvGEabZa"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Akhir\n",
        "df[\"WER_final\"] = df.apply(lambda r: wer(r[\"ref_clean\"], r[\"hyp_clean\"]), axis=1)\n",
        "df[\"CER_final\"] = df.apply(lambda r: cer(r[\"ref_clean\"], r[\"hyp_clean\"]), axis=1)\n",
        "\n",
        "# Global Metrics\n",
        "global_wer = wer(\" \".join(df.ref_clean), \" \".join(df.hyp_clean))\n",
        "global_cer = cer(\" \".join(df.ref_clean), \" \".join(df.hyp_clean))"
      ],
      "metadata": {
        "id": "Utqmp8jnJsZ-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Evaluasi per Audio\n",
        "print(\"\\nHasil per-audio:\")\n",
        "print(df[[\"index\", \"WER_final\", \"CER_final\"]].rename(columns={\"WER_final\": \"WER\", \"CER_final\": \"CER\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpq48HK4Szma",
        "outputId": "c39fb60c-ad34-49c5-9a6f-8c5a2145339b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hasil per-audio:\n",
            "   index       WER       CER\n",
            "0      1  0.626866  0.317523\n",
            "1      2  0.636364  0.313175\n",
            "2      3  0.721519  0.372352\n",
            "3      4  0.500000  0.256410\n",
            "4      5  0.617225  0.322191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Global WER:\", global_wer)\n",
        "print(\"Global CER:\", global_cer)\n",
        "print(\"Akurasi STT (WER):\", 1 - global_wer)\n",
        "print(\"Akurasi STT (CER):\", 1 - global_cer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQiVTH8JQ8UV",
        "outputId": "d146b554-82f3-4361-df05-e3420954a65c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global WER: 0.6123399301513388\n",
            "Global CER: 0.3142493638676845\n",
            "Akurasi STT (WER): 0.3876600698486612\n",
            "Akurasi STT (CER): 0.6857506361323156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan hasil\n",
        "df.to_csv(\"final_transcript_metrics_wav2vec2.csv\", index=False)"
      ],
      "metadata": {
        "id": "KEuBmQj_Q-aG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perbandingan Whisper dan WAV2VEC2"
      ],
      "metadata": {
        "id": "s22wl3P6eaig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_whisper = pd.read_csv(\"/content/final_transcript_metrics.csv\")            # punya WER whisper\n",
        "df_w2v2    = pd.read_csv(\"/content/final_transcript_metrics_wav2vec2.csv\")   # punya WER wav2vec2\n",
        "\n",
        "# Merge berdasarkan index\n",
        "df_compare = df_whisper[[\"index\", \"WER_final\", \"CER_final\"]].merge(\n",
        "    df_w2v2[[\"index\", \"WER_final\", \"CER_final\"]],\n",
        "    on=\"index\",\n",
        "    suffixes=(\"_whisper\", \"_wav2vec2\"))\n",
        "\n",
        "print(\"Perbandingan WER & CER Whisper dan WAV2VEC2\")\n",
        "df_compare\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "8E8TuWVIh6pq",
        "outputId": "c1c954ec-57fc-4aea-d09b-714cc09c67af"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perbandingan WER & CER Whisper dan WAV2VEC2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   index  WER_final_whisper  CER_final_whisper  WER_final_wav2vec2  \\\n",
              "0      1           0.089552           0.067194            0.626866   \n",
              "1      2           0.084416           0.039957            0.636364   \n",
              "2      3           0.113924           0.079153            0.721519   \n",
              "3      4           0.083333           0.063628            0.500000   \n",
              "4      5           0.091346           0.082166            0.617225   \n",
              "\n",
              "   CER_final_wav2vec2  \n",
              "0            0.317523  \n",
              "1            0.313175  \n",
              "2            0.372352  \n",
              "3            0.256410  \n",
              "4            0.322191  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c27dc30f-e4b7-4485-9f9d-0198205240f2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>WER_final_whisper</th>\n",
              "      <th>CER_final_whisper</th>\n",
              "      <th>WER_final_wav2vec2</th>\n",
              "      <th>CER_final_wav2vec2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.089552</td>\n",
              "      <td>0.067194</td>\n",
              "      <td>0.626866</td>\n",
              "      <td>0.317523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.084416</td>\n",
              "      <td>0.039957</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.313175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.113924</td>\n",
              "      <td>0.079153</td>\n",
              "      <td>0.721519</td>\n",
              "      <td>0.372352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.063628</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.256410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.091346</td>\n",
              "      <td>0.082166</td>\n",
              "      <td>0.617225</td>\n",
              "      <td>0.322191</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c27dc30f-e4b7-4485-9f9d-0198205240f2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c27dc30f-e4b7-4485-9f9d-0198205240f2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c27dc30f-e4b7-4485-9f9d-0198205240f2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36d3af87-afb6-4b0f-809a-a24eff6c931d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36d3af87-afb6-4b0f-809a-a24eff6c931d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36d3af87-afb6-4b0f-809a-a24eff6c931d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e1058752-f768-4d58-abc0-c91eaca21a7f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_compare')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e1058752-f768-4d58-abc0-c91eaca21a7f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_compare');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_compare",
              "summary": "{\n  \"name\": \"df_compare\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WER_final_whisper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01243377594664236,\n        \"min\": 0.0833333333333333,\n        \"max\": 0.1139240506329113,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0844155844155844,\n          0.0913461538461538,\n          0.1139240506329113\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CER_final_whisper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01672394518192485,\n        \"min\": 0.0399568034557235,\n        \"max\": 0.0821661998132586,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0399568034557235,\n          0.0821661998132586,\n          0.0791527313266443\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WER_final_wav2vec2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07910109581641511,\n        \"min\": 0.5,\n        \"max\": 0.7215189873417721,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.6363636363636364,\n          0.6172248803827751,\n          0.7215189873417721\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CER_final_wav2vec2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04115399249200092,\n        \"min\": 0.2564102564102564,\n        \"max\": 0.3723522853957636,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.3131749460043196,\n          0.3221912720519962,\n          0.3723522853957636\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pada tahap evaluasi model Speech-to-Text (STT), dilakukan perbandingan antara OpenAI Whisper dan Facebook Wav2Vec2 menggunakan dua metrik utama:\n",
        "* **WER (Word Error Rate)** — mengukur kesalahan pada tingkat kata\n",
        "\n",
        "* **CER (Character Error Rate)** — mengukur kesalahan pada tingkat karakter\n",
        "\n",
        "Semakin rendah nilai WER/CER, semakin akurat model tersebut dalam mentranskripsi audio.\n",
        "\n",
        "Dataset yang digunakan untuk evaluasi terdiri dari 5 video dengan durasi rata-rata ±2 menit (total ±10 menit audio)."
      ],
      "metadata": {
        "id": "-1UTzipwjL80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANALISIS**\n",
        "\n",
        "Dari ke-5 data, Whisper mengungguli WAV2VEC2 pada seluruh data. Hal ini menunjukkan bahwa Whisper mampu menghasilkan transkripsi yang lebih mendekati ground truth pada seluruh sampel.\n",
        "\n",
        "Nilai error yang tinggi pada Wav2Vec2 (bahkan lebih dari 5–8 kali lipat Whisper) menunjukkan bahwa model ini:\n",
        "1. Kurang stabil untuk interview\n",
        "2. Lebih sensitif terhadap intonasi dan jeda\n",
        "3. Berpotensi salah mengenali kata, membuang kata, atau menambahkan kata yang tidak diucapkan\n",
        "\n",
        "**KESIMPULAN**\n",
        "Berdasarkan metrik WER & CER, dapat disimpulkan bahwa Whisper dipilih sebagai model STT untuk sistem AI Interview Assessment"
      ],
      "metadata": {
        "id": "1jAdXYWmjh5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STT + Groq API Untuk Penilaian Otomatis Based On Rubrics"
      ],
      "metadata": {
        "id": "0iKbO2tzBvsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load JSON Rubrics Interview\n",
        "# Download rubrics_interview.json dari Google Drive\n",
        "!gdown --id 1METGlHn1m5ZOuSrNKL8NVsvIn6zKLSF9 -O rubrics_interview.json\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcYCuA5GRBTr",
        "outputId": "87ceffc5-728f-4d9a-e864-5d2fd309a394"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1METGlHn1m5ZOuSrNKL8NVsvIn6zKLSF9\n",
            "To: /content/rubrics_interview.json\n",
            "100% 6.55k/6.55k [00:00<00:00, 25.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load transcript hasil Whisper\n",
        "df = pd.read_csv(\"final_transcript_metrics.csv\")\n",
        "\n",
        "# Load rubrik penilaian\n",
        "with open(\"rubrics_interview.json\") as f:\n",
        "    rubrics = json.load(f)"
      ],
      "metadata": {
        "id": "dGMVgsWwOCnm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(rubrics['1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxl6aWWPSpJj",
        "outputId": "cf8f8688-e04c-4041-83b4-bc5334a08617"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'Can you share any specific challenges you faced while working on '\n",
            "             'certification and how you overcame them?',\n",
            " 'rubric': {'0': ['Unanswered'],\n",
            "            '1': ['Minimal or Vague Response'],\n",
            "            '2': ['General Challenge Mentioned without Details'],\n",
            "            '3': ['Describes at least one specific challenge related to '\n",
            "                  'building machine learning models with TensorFlow.',\n",
            "                  'Provides a basic explanation of how the challenge was '\n",
            "                  'overcome.',\n",
            "                  'Explanation may be brief or lack depth.',\n",
            "                  'Shows some understanding but lacks detailed insight.'],\n",
            "            '4': ['Provides a detailed description of specific challenges '\n",
            "                  'encountered during the certification.',\n",
            "                  'Offers clear explanations of how each challenge was '\n",
            "                  'overcome.',\n",
            "                  'Demonstrates strong understanding of technical aspects and '\n",
            "                  'problem-solving skills.',\n",
            "                  'Response is very clear, well-organized, and reflects on the '\n",
            "                  'learning process.']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Groq client\n",
        "client = Groq(api_key=\"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\") #masukkan atau ganti api key\n",
        "\n",
        "print(\"Groq client initialized!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEnn9m49-HAJ",
        "outputId": "3780b4d6-06e0-4132-ffb5-b4e42900e5bd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq client initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(question: str, answer: str, rubric: dict) -> str:\n",
        "    # Format rubrik menjadi teks terstruktur\n",
        "    rubric_text = \"\"\n",
        "    for score in sorted(rubric.keys(), key=int):\n",
        "        descriptions = rubric[score]\n",
        "        rubric_text += f\"\\nScore {score}:\\n\"\n",
        "        for desc in descriptions:\n",
        "            rubric_text += f\"- {desc}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an evaluator. Assess the following candidate response to the interview question.\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER:\n",
        "{answer}\n",
        "\n",
        "SCORING RUBRIC:\n",
        "{rubric_text}\n",
        "\n",
        "Please evaluate the answer based on the rubric and respond ONLY in JSON format:\n",
        "{{\n",
        "  \"score\": (a number from 0 to 4),\n",
        "  \"reason\": (a short explanation why the score was given)\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "IzDdhYPyTvRz"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_evaluate(prompt: str) -> str:\n",
        "    try:\n",
        "        chat_completion = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful interview evaluator. Always respond in valid JSON format.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            temperature=0,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "\n",
        "        return chat_completion.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"score\": None, \"reason\": f\"Error: {str(e)}\"})"
      ],
      "metadata": {
        "id": "YqbX2jInT3fr"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    idx = str(row[\"index\"])\n",
        "    question = rubrics[idx][\"question\"]\n",
        "    rubric = rubrics[idx][\"rubric\"]\n",
        "    answer = row[\"hyp_no_filler\"]\n",
        "\n",
        "    print(f\"\\nMenilai video ke-{idx}...\")\n",
        "\n",
        "    # Generate prompt\n",
        "    prompt = make_prompt(question, answer, rubric)\n",
        "\n",
        "    try:\n",
        "        # Mendapatkan respons dari Groq\n",
        "        output = llm_evaluate(prompt)\n",
        "        print(\"Output LLM:\", output[:300])\n",
        "\n",
        "        # Mencari JSON object di dalam response\n",
        "        if \"{\" in output:\n",
        "            json_str = output[output.find(\"{\"):output.rfind(\"}\")+1]\n",
        "            parsed = json.loads(json_str)\n",
        "            score = parsed.get(\"score\", None)\n",
        "            reason = parsed.get(\"reason\", \"\")\n",
        "        else:\n",
        "            score = None\n",
        "            reason = \"No valid JSON found in response\"\n",
        "\n",
        "    except Exception as e:\n",
        "        score = None\n",
        "        reason = f\"Gagal parsing: {e}\"\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    results.append({\n",
        "        \"index\": row[\"index\"],\n",
        "        \"score\": score,\n",
        "        \"reason\": reason\n",
        "    })\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\nEvaluasi selesai!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWIVTs3AT40W",
        "outputId": "a6cc180f-0302-4639-fb2c-32c5a44a2001"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Menilai video ke-1...\n",
            "Output LLM: {\n",
            "  \"score\": 3,\n",
            "  \"reason\": \"The candidate describes a specific challenge related to meeting the accuracy or validation loss for the evaluation matrix and provides a basic explanation of how it was overcome, such as trying different architectures and applying dropout layers, but the explanation lack\n",
            "\n",
            "Menilai video ke-2...\n",
            "Output LLM: {\n",
            "  \"score\": 2,\n",
            "  \"reason\": \"The candidate mentions transfer learning and TensorFlow, provides some examples of models like VGG16 and VGG19, and acknowledges the benefits of using transfer learning, but lacks specific personal experience and detailed explanations of how it benefited their projects.\"\n",
            "\n",
            "Menilai video ke-3...\n",
            "Output LLM: {\n",
            "  \"score\": 3,\n",
            "  \"reason\": \"The candidate describes a specific TensorFlow model they built for celiac disease prediction, mentions its complexity, and explains steps taken to ensure accuracy, such as using SMOTE and synthetic oversampling. However, the explanation lacks depth and specificity regard\n",
            "\n",
            "Menilai video ke-4...\n",
            "Output LLM: {\n",
            "  \"score\": 2,\n",
            "  \"reason\": \"The candidate provides a general response with limited details, mentioning the use of a dropout layer and its effect on training, but lacks specifics and clarity in their explanation.\"\n",
            "}\n",
            "\n",
            "Menilai video ke-5...\n",
            "Output LLM: {\n",
            "  \"score\": 3,\n",
            "  \"reason\": \"The candidate describes the general process of building a CNN in TensorFlow, including data preprocessing, defining the CNN architecture, and key layers, but lacks comprehensive detail and omits important aspects such as compiling the model and evaluating performance.\"\n",
            "}\n",
            "\n",
            "Evaluasi selesai!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format ke payload JSON\n",
        "interview_scores = []\n",
        "\n",
        "for item in results:\n",
        "    interview_scores.append({\n",
        "        \"id\": int(item[\"index\"]),\n",
        "        \"score\": item[\"score\"],\n",
        "        \"reason\": item[\"reason\"]\n",
        "    })\n",
        "\n",
        "# Hitung rata-rata score (skip yang None)\n",
        "valid_scores = [x[\"score\"] for x in interview_scores if x[\"score\"] is not None]\n",
        "avg_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0\n",
        "\n",
        "project_score = 100\n",
        "interview_normalized = (avg_score / 4) * 100\n",
        "total_score = (project_score + interview_normalized) / 2\n",
        "\n",
        "payload = {\n",
        "    \"assessorProfile\": {\n",
        "        \"id\": 0,\n",
        "        \"name\": \"AI Assessor (Groq Mixtral-8x7B)\",\n",
        "        \"photoUrl\": \"\"\n",
        "    },\n",
        "    \"decision\": \"Pending\",\n",
        "    \"reviewedAt\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"scoresOverview\": {\n",
        "        \"project\": round(project_score, 2),\n",
        "        \"interview\": round(avg_score, 2),\n",
        "        \"interview_normalized\": round(interview_normalized, 2),\n",
        "        \"total\": round(total_score, 2)\n",
        "    },\n",
        "    \"reviewChecklistResult\": {\n",
        "        \"project\": [],\n",
        "        \"interviews\": {\n",
        "            \"minScore\": 0,\n",
        "            \"maxScore\": 4,\n",
        "            \"scores\": interview_scores\n",
        "        }\n",
        "    },\n",
        "    \"overallNotes\": \"Generated automatically by Groq API based on rubric scoring.\"\n",
        "}\n",
        "\n",
        "# Simpan ke file JSON\n",
        "with open(\"interview_scores_groq.json\", \"w\") as f:\n",
        "    json.dump(payload, f, indent=2)\n",
        "\n",
        "print(\"\\nPayload penilaian disimpan ke interview_scores_groq.json\")\n",
        "print(f\"Average Interview Score: {avg_score:.2f}/4\")\n",
        "print(f\"Interview Normalized: {interview_normalized:.2f}/100\")\n",
        "print(f\"Total Score: {total_score:.2f}/100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yOwxImI-NUn",
        "outputId": "bc2eb820-840f-499c-a37c-c37096c9e535"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Payload penilaian disimpan ke interview_scores_groq.json\n",
            "Average Interview Score: 2.60/4\n",
            "Interview Normalized: 65.00/100\n",
            "Total Score: 82.50/100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MERGE JSON OUTPUT"
      ],
      "metadata": {
        "id": "pvLDVbBjAKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# File JSON kamu\n",
        "transcript_file = \"transcripts.json\"\n",
        "score_file = \"interview_scores_groq.json\"\n",
        "\n",
        "# Baca transcripts.json\n",
        "with open(transcript_file, \"r\") as f:\n",
        "    transcripts = json.load(f)\n",
        "\n",
        "# Baca interview_scores_groq.json\n",
        "with open(score_file, \"r\") as f:\n",
        "    interview_scores = json.load(f)\n",
        "\n",
        "# Gabungkan dalam satu struktur yang rapi\n",
        "merged = {\n",
        "    \"transcripts\": transcripts,\n",
        "    \"interview_scores\": interview_scores\n",
        "}\n",
        "\n",
        "# Simpan file gabungan\n",
        "with open(\"final_payload.json\", \"w\") as f:\n",
        "    json.dump(merged, f, indent=2)\n",
        "\n",
        "print(\"final_payload.json berhasil dibuat!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brnyN-RMAOzM",
        "outputId": "fe682872-12b8-48fe-b65c-d2930943cd80"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final_payload.json berhasil dibuat!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "Q1Xjl_xK-1f3"
      },
      "execution_count": 62,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "-wLTL-0SShkW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1807044904a41f2abc3cf72f0a5fe3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d99920002ca6497686f4e9c747f83b37",
              "IPY_MODEL_e12acd48fdba4ca8a5a5ca9b641d7655",
              "IPY_MODEL_7d33a7fbf4af48909bdfdf45fd6f4845"
            ],
            "layout": "IPY_MODEL_a0344a7cbcc74d1182522337ea5ad7e0"
          }
        },
        "d99920002ca6497686f4e9c747f83b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_372f014095ea4916aea4199b28c9a68c",
            "placeholder": "​",
            "style": "IPY_MODEL_2e9e4506c27b47c4a263274a50982b53",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "e12acd48fdba4ca8a5a5ca9b641d7655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1844695c258a4860a031ba7d2faa5b5c",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6656d90fbc784fc9a85f1c82644790ea",
            "value": 212
          }
        },
        "7d33a7fbf4af48909bdfdf45fd6f4845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76567c377cb649c9a3fb7c0a98a3edbd",
            "placeholder": "​",
            "style": "IPY_MODEL_b094df04850c4bb3bda621b574f9d1fe",
            "value": " 212/212 [00:00&lt;00:00, 15.1kB/s]"
          }
        },
        "a0344a7cbcc74d1182522337ea5ad7e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372f014095ea4916aea4199b28c9a68c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e9e4506c27b47c4a263274a50982b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1844695c258a4860a031ba7d2faa5b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6656d90fbc784fc9a85f1c82644790ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76567c377cb649c9a3fb7c0a98a3edbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b094df04850c4bb3bda621b574f9d1fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ae46c81d9e7426da1987460d9759d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_463ea277f57b403ca32849824e531995",
              "IPY_MODEL_00e27d6e3b264eca8581e2d5e0f8b199",
              "IPY_MODEL_63a8bfbd69c748f69a5ea55d819f1350"
            ],
            "layout": "IPY_MODEL_bac163bad1f5411f99ee5d1146abac2e"
          }
        },
        "463ea277f57b403ca32849824e531995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352ba84b623346768ea2f7beec6f8de9",
            "placeholder": "​",
            "style": "IPY_MODEL_ddc055ee5c3e46e39724103e2808e8b1",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "00e27d6e3b264eca8581e2d5e0f8b199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daf4d51e45514ffebfc24e03a30cb7b4",
            "max": 181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ca117f287b24d4c828916e6dd001214",
            "value": 181
          }
        },
        "63a8bfbd69c748f69a5ea55d819f1350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db37436578394ddaa9adc8c78114d944",
            "placeholder": "​",
            "style": "IPY_MODEL_db081be90a814ba79b805059a51a0b90",
            "value": " 181/181 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "bac163bad1f5411f99ee5d1146abac2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "352ba84b623346768ea2f7beec6f8de9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddc055ee5c3e46e39724103e2808e8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf4d51e45514ffebfc24e03a30cb7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca117f287b24d4c828916e6dd001214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db37436578394ddaa9adc8c78114d944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db081be90a814ba79b805059a51a0b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea683f1cb5ef450da357f91137fd287d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1cef9eec5904f559b8db1e2142ccfab",
              "IPY_MODEL_42cdb2dcbf804d91b30629772b481971",
              "IPY_MODEL_01a80e86777a4e0eb5ec9365e6ddea6d"
            ],
            "layout": "IPY_MODEL_e638b519403c4bfb837d335b5ec86374"
          }
        },
        "c1cef9eec5904f559b8db1e2142ccfab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20eb1d238ef493794de709dcccff43c",
            "placeholder": "​",
            "style": "IPY_MODEL_7d45ab9f07d94a6ca50e881c44d0a669",
            "value": "vocab.json: 100%"
          }
        },
        "42cdb2dcbf804d91b30629772b481971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35ae89b38f044213b590d13435d89994",
            "max": 292,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c793fdc26ec7423e95173276ba482bd4",
            "value": 292
          }
        },
        "01a80e86777a4e0eb5ec9365e6ddea6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eefc3e5df4b4ad8812f52e030fc7246",
            "placeholder": "​",
            "style": "IPY_MODEL_9f0de277452542bda5d92eb1bce300c8",
            "value": " 292/292 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "e638b519403c4bfb837d335b5ec86374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20eb1d238ef493794de709dcccff43c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d45ab9f07d94a6ca50e881c44d0a669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ae89b38f044213b590d13435d89994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c793fdc26ec7423e95173276ba482bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2eefc3e5df4b4ad8812f52e030fc7246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0de277452542bda5d92eb1bce300c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f5710e0dc0c4ce7809d8269f092d18c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_08fc3d6838244b5ba6ec0b5554ce1f3e",
              "IPY_MODEL_d4441be2fb504a34ba0390568ec7f59a",
              "IPY_MODEL_d9e294727e8643f4a5693b4954a74d59"
            ],
            "layout": "IPY_MODEL_4fddd4fe7d4c4050a2b7f686c2166909"
          }
        },
        "08fc3d6838244b5ba6ec0b5554ce1f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e355d5cc90f4653892a66aa74be9993",
            "placeholder": "​",
            "style": "IPY_MODEL_e26487ebdde346a6a9fb483ea78966c0",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d4441be2fb504a34ba0390568ec7f59a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed1a7ae0e3f49418f9c5d7b25550fe1",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f61a471a0b9249f0a886f5c05486bab6",
            "value": 85
          }
        },
        "d9e294727e8643f4a5693b4954a74d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2ca43ad9a074272af417dcb3e0103fd",
            "placeholder": "​",
            "style": "IPY_MODEL_b24cb278180d4272b2bdbbe4e8a510d6",
            "value": " 85.0/85.0 [00:00&lt;00:00, 7.99kB/s]"
          }
        },
        "4fddd4fe7d4c4050a2b7f686c2166909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e355d5cc90f4653892a66aa74be9993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26487ebdde346a6a9fb483ea78966c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ed1a7ae0e3f49418f9c5d7b25550fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61a471a0b9249f0a886f5c05486bab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2ca43ad9a074272af417dcb3e0103fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24cb278180d4272b2bdbbe4e8a510d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7130950fea8e47158b63b9f30c35197c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3f7c54bca72477992086e029ce36ae5",
              "IPY_MODEL_ba6f41c203a346b1945aeedc909059bc",
              "IPY_MODEL_76be6ac62ec845f9a7b8c72b75ff202b"
            ],
            "layout": "IPY_MODEL_f52b232b84324fb3a28548b67f87b6a1"
          }
        },
        "e3f7c54bca72477992086e029ce36ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_499123e471284a618e259be5f7de12be",
            "placeholder": "​",
            "style": "IPY_MODEL_07783045f4ca46348c5f43b4f52ee8d9",
            "value": "config.json: "
          }
        },
        "ba6f41c203a346b1945aeedc909059bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be9a55668854fc19739011f27f761cb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7cba12b395e42bea661f583043fa5a1",
            "value": 1
          }
        },
        "76be6ac62ec845f9a7b8c72b75ff202b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_281d716053e8421c914083e2193509f9",
            "placeholder": "​",
            "style": "IPY_MODEL_18ba1e8e9d324ca1a3760850ba64e23c",
            "value": " 1.57k/? [00:00&lt;00:00, 127kB/s]"
          }
        },
        "f52b232b84324fb3a28548b67f87b6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "499123e471284a618e259be5f7de12be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07783045f4ca46348c5f43b4f52ee8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6be9a55668854fc19739011f27f761cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d7cba12b395e42bea661f583043fa5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "281d716053e8421c914083e2193509f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ba1e8e9d324ca1a3760850ba64e23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e079d6875764a0c81342575012b1622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5d6411ed78546f0b10aad0ce1c3238b",
              "IPY_MODEL_e031c8740bee4a2a99ac628c98fd7148",
              "IPY_MODEL_a8ae2cdd61554d0fa70c346563c46e51"
            ],
            "layout": "IPY_MODEL_c50e6e43e48447bf90712cfe46641ad1"
          }
        },
        "e5d6411ed78546f0b10aad0ce1c3238b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b9b24bb6b746509aaa03e7f6ea4943",
            "placeholder": "​",
            "style": "IPY_MODEL_cef1b99d22d845f784cfbc3b2ee1abb7",
            "value": "model.safetensors: 100%"
          }
        },
        "e031c8740bee4a2a99ac628c98fd7148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bfad4c2f37a48ea8ffa0fe0acc32aa7",
            "max": 1261938632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_043d5ebecea14b2a90c75d82cf77a4f6",
            "value": 1261938632
          }
        },
        "a8ae2cdd61554d0fa70c346563c46e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faec63d08ff84670941534a3c1cb228f",
            "placeholder": "​",
            "style": "IPY_MODEL_96f46443b1534c05ac53c0de7e8a896a",
            "value": " 1.26G/1.26G [00:21&lt;00:00, 75.7MB/s]"
          }
        },
        "c50e6e43e48447bf90712cfe46641ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55b9b24bb6b746509aaa03e7f6ea4943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef1b99d22d845f784cfbc3b2ee1abb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bfad4c2f37a48ea8ffa0fe0acc32aa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "043d5ebecea14b2a90c75d82cf77a4f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faec63d08ff84670941534a3c1cb228f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96f46443b1534c05ac53c0de7e8a896a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}